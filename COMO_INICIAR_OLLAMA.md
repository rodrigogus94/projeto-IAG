# üöÄ Como Iniciar o Ollama - Guia R√°pido

## üì• Passo 1: Instalar o Ollama

Se o Ollama n√£o est√° instalado:

1. **Acesse**: https://ollama.ai/download
2. **Baixe** o instalador para Windows
3. **Execute** o instalador e siga as instru√ß√µes
4. O Ollama ser√° instalado e adicionado ao PATH automaticamente

---

## ‚ñ∂Ô∏è Passo 2: Iniciar o Ollama

### Op√ß√£o A: Pelo Menu Iniciar (Mais F√°cil) ‚≠ê

1. Pressione a tecla **Windows**
2. Digite **"Ollama"**
3. Clique no aplicativo **Ollama**
4. O Ollama iniciar√° automaticamente em segundo plano

### Op√ß√£o B: Pelo Terminal/PowerShell

1. Abra o **PowerShell** ou **Prompt de Comando**
2. Execute:
   ```powershell
   ollama serve
   ```
3. Deixe o terminal aberto (o Ollama rodar√° enquanto o terminal estiver aberto)
4. Voc√™ ver√° uma mensagem como: `Ollama is running on http://localhost:11434`

---

## ‚úÖ Passo 3: Verificar se Est√° Funcionando

### Teste 1: Verificar se est√° rodando
```powershell
ollama list
```
Se retornar uma lista (mesmo que vazia), o Ollama est√° rodando! ‚úÖ

### Teste 2: Testar a API
No navegador, acesse:
```
http://localhost:11434/api/tags
```
Deve retornar um JSON com os modelos dispon√≠veis.

### Teste 3: Usando o script de diagn√≥stico do projeto
```powershell
python scripts/diagnose_ollama.py
```

---

## üì¶ Passo 4: Baixar um Modelo (Obrigat√≥rio)

Se n√£o houver modelos instalados, voc√™ precisa baixar pelo menos um:

```powershell
# Modelo recomendado para come√ßar
ollama pull llama2

# Ou outros modelos populares:
ollama pull mistral
ollama pull codellama
ollama pull phi
```

**Nota**: O primeiro download pode demorar alguns minutos dependendo do tamanho do modelo.

---

## üîß Passo 5: Configurar o Projeto

Ap√≥s iniciar o Ollama:

1. **Inicie a aplica√ß√£o Streamlit**:
   ```powershell
   streamlit run src/app.py
   ```

2. **Na sidebar**, expanda "‚öôÔ∏è Configura√ß√µes"

3. **Clique em "üîÑ Reconectar ao Ollama"**

4. O status deve mudar para "‚úÖ Conectado ao Ollama"

---

## üêõ Problemas Comuns

### ‚ùå "Ollama n√£o √© reconhecido como comando"

**Solu√ß√£o**:
- O Ollama n√£o est√° instalado ou n√£o est√° no PATH
- Reinstale o Ollama de https://ollama.ai/download
- Reinicie o terminal ap√≥s instalar

### ‚ùå "Porta 11434 j√° est√° em uso"

**Solu√ß√£o**:
- Outro processo est√° usando a porta
- Feche outros programas que possam estar usando a porta
- Ou reinicie o computador

### ‚ùå "Imposs√≠vel conectar-se ao servidor remoto"

**Solu√ß√£o**:
- O Ollama n√£o est√° rodando
- Inicie o Ollama usando uma das op√ß√µes acima
- Verifique se o firewall n√£o est√° bloqueando

### ‚ùå "Nenhum modelo encontrado"

**Solu√ß√£o**:
- Baixe pelo menos um modelo:
  ```powershell
  ollama pull llama2
  ```

---

## üí° Dicas

### Iniciar o Ollama Automaticamente ao Ligar o Computador

1. Pressione `Win + R`
2. Digite `shell:startup` e pressione Enter
3. Crie um atalho do Ollama nesta pasta
4. O Ollama iniciar√° automaticamente ao iniciar o Windows

### Verificar Modelos Instalados

```powershell
ollama list
```

### Remover um Modelo

```powershell
ollama rm nome_do_modelo
```

### Ver Informa√ß√µes de um Modelo

```powershell
ollama show nome_do_modelo
```

---

## üìö Recursos Adicionais

- **Documenta√ß√£o oficial**: https://github.com/ollama/ollama
- **Lista de modelos**: https://ollama.ai/library
- **Documenta√ß√£o do projeto**: `docs/INICIAR_OLLAMA.md`

---

## ‚úÖ Checklist R√°pido

- [ ] Ollama instalado
- [ ] Ollama rodando (verificar com `ollama list`)
- [ ] Pelo menos um modelo baixado (`ollama pull llama2`)
- [ ] Aplica√ß√£o Streamlit iniciada
- [ ] Conectado ao Ollama nas configura√ß√µes

---

**Pronto! Agora voc√™ pode usar o chat com IA! üéâ**



