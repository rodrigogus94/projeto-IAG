# Chat Assistente com IA - Projeto IAG

Aplica√ß√£o web de chat interativo com IA usando Streamlit e Ollama. Interface moderna e intuitiva para conversar com modelos de linguagem locais atrav√©s do Ollama, com suporte para transcri√ß√£o de √°udio.

## Caracter√≠sticas

- **Interface Moderna**: Interface web responsiva constru√≠da com Streamlit
- **Ollama Integration**: Suporte completo para modelos locais via Ollama
- **M√∫ltiplos Modelos**: Lista dinamicamente modelos dispon√≠veis no Ollama
- **Transcri√ß√£o de √Åudio**: Suporte para entrada por voz usando Whisper (local) ou OpenAI API
- **Hist√≥rico Completo**: Mant√©m contexto completo da conversa
- **Configura√ß√£o Flex√≠vel**: Suporte para vari√°veis de ambiente (.env) ou configura√ß√£o manual
- **Arquitetura Modular**: C√≥digo organizado e separado por responsabilidades

## Pr√©-requisitos

- Python 3.8 ou superior
- Ollama instalado e rodando (https://ollama.ai/)
- pip (gerenciador de pacotes Python)
- (Opcional) OpenAI API Key para transcri√ß√£o de √°udio via API

## Instala√ß√£o

1. **Clone ou baixe o projeto**

2. **Instale o Ollama** (se ainda n√£o tiver):

   - Windows/Mac: Baixe de https://ollama.ai/
   - Linux: `curl -fsSL https://ollama.ai/install.sh | sh`

3. **Baixe um modelo do Ollama**:

   ```bash
   ollama pull llama2
   # ou
   ollama pull mistral
   # ou qualquer outro modelo dispon√≠vel
   ```

4. **Instale as depend√™ncias do Python**:

   ```bash
   pip install -r requirements.txt
   ```

5. **Configure vari√°veis de ambiente (opcional)**:

   Crie um arquivo `.env` na raiz do projeto:

   ```env
   OLLAMA_BASE_URL=http://localhost:11434
   TRANSCRIPTION_METHOD=whisper
   # Opcional: para transcri√ß√£o via OpenAI API
   OPENAI_API_KEY=sk-sua-chave-api-aqui
   ```

## Como Usar

1. **Certifique-se de que o Ollama est√° rodando**:

   ```bash
   ollama serve
   ```

2. **Inicie a aplica√ß√£o**:

   ```bash
   streamlit run app.py
   ```

3. **Configure a conex√£o**:

   - Na sidebar, expanda "‚öôÔ∏è Configura√ß√µes"
   - Verifique se a URL do Ollama est√° correta (padr√£o: http://localhost:11434)
   - Clique em "üîÑ Reconectar ao Ollama" se necess√°rio
   - Selecione o modelo desejado (ser√° listado automaticamente)

4. **Comece a conversar**:
   - Digite sua mensagem no campo de input
   - Ou use o microfone para gravar uma mensagem de voz
   - A IA responder√° mantendo o contexto da conversa
   - Use "üóëÔ∏è Limpar Chat" para reiniciar a conversa

## Configura√ß√µes

### Modelos Ollama

O aplicativo lista automaticamente os modelos dispon√≠veis no seu Ollama. Para baixar novos modelos:

```bash
ollama pull llama2
ollama pull mistral
ollama pull codellama
# etc.
```

### Transcri√ß√£o de √Åudio

Dois m√©todos dispon√≠veis:

1. **Whisper Local** (padr√£o):

   - Usa `openai-whisper` instalado localmente
   - N√£o requer API Key
   - Processa localmente (pode ser mais lento)

2. **OpenAI API**:
   - Usa a API da OpenAI para transcri√ß√£o
   - Requer `OPENAI_API_KEY` no `.env`
   - Mais r√°pido, mas requer conex√£o com internet

Configure no menu de configura√ß√µes ou via vari√°vel de ambiente `TRANSCRIPTION_METHOD`.

### Par√¢metros

- **Temperature**: Controla a criatividade (0.0 = determin√≠stico, 2.0 = muito criativo)
- **Modelo**: Selecione entre os modelos dispon√≠veis no Ollama

## Estrutura do Projeto

```
projeto-sdk-mk01/
‚îú‚îÄ‚îÄ app.py              # Interface Streamlit principal
‚îú‚îÄ‚îÄ llm_handler.py      # Handler que integra OllamaService
‚îú‚îÄ‚îÄ ollama_service.py    # Servi√ßo para comunica√ß√£o com Ollama
‚îú‚îÄ‚îÄ audio_transcriber.py # M√≥dulo de transcri√ß√£o de √°udio
‚îú‚îÄ‚îÄ model_config.py     # Configura√ß√µes centralizadas do modelo (regras, par√¢metros, prompts)
‚îú‚îÄ‚îÄ styles.py           # Estilos CSS customizados
‚îú‚îÄ‚îÄ requirements.txt    # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ .env                # Vari√°veis de ambiente (criar)
‚îî‚îÄ‚îÄ README.md           # Este arquivo
```

## Arquitetura

O projeto segue uma arquitetura modular:

- **`app.py`**: Gerencia a interface do usu√°rio, estado da aplica√ß√£o e intera√ß√µes
- **`llm_handler.py`**: Adapta OllamaService para a interface esperada pelo app
- **`ollama_service.py`**: Encapsula toda a l√≥gica de comunica√ß√£o com a API do Ollama
- **`audio_transcriber.py`**: Gerencia transcri√ß√£o de √°udio (Whisper/OpenAI)
- **`model_config.py`**: **Centraliza todas as configura√ß√µes do modelo** - regras, par√¢metros, system prompts e instru√ß√µes
- **`styles.py`**: Centraliza todos os estilos CSS customizados

### Configura√ß√£o do Modelo (`model_config.py`)

O arquivo `model_config.py` √© o **centro de controle** para todas as configura√ß√µes do modelo:

- **System Prompts**: Persona e instru√ß√µes do assistente
- **Par√¢metros Padr√£o**: Temperatura, modelo padr√£o, limites
- **Regras de Comportamento**: Como o modelo deve se comportar
- **Prompts por Contexto**: Instru√ß√µes espec√≠ficas para diferentes situa√ß√µes
- **Valida√ß√µes**: Regras de valida√ß√£o de inputs
- **Configura√ß√µes Avan√ßadas**: Retry, cache, logging, etc.

**Para personalizar o comportamento do modelo**, edite o arquivo `model_config.py`:

- Ajuste o `SYSTEM_PROMPT` para mudar a persona do assistente
- Modifique `DEFAULT_TEMPERATURE` para alterar a criatividade padr√£o
- Adicione novos contextos em `CONTEXT_PROMPTS`
- Configure regras de valida√ß√£o em `VALIDATION_RULES`

## Solu√ß√£o de Problemas

### Ollama n√£o conecta

1. **Execute o script de diagn√≥stico**:

   ```bash
   python diagnose_ollama.py
   ```

   Este script verifica automaticamente todos os aspectos da conex√£o.

2. **Verifique se o Ollama est√° rodando**:

   ```bash
   ollama list
   ```

   Se retornar erro, inicie o Ollama:

   ```bash
   ollama serve
   ```

3. **Verifique a URL nas configura√ß√µes**:

   - Padr√£o: `http://localhost:11434`
   - Se estiver usando Docker: `http://localhost:11434` (ou a porta configurada)
   - Se estiver em servidor remoto: `http://IP_DO_SERVIDOR:11434`

4. **Verifique o firewall**:

   - O Ollama usa a porta 11434 por padr√£o
   - Certifique-se de que a porta n√£o est√° bloqueada

5. **Teste manualmente a API**:
   ```bash
   curl http://localhost:11434/api/tags
   ```
   Deve retornar uma lista de modelos em JSON.

### Nenhum modelo dispon√≠vel

1. Baixe pelo menos um modelo:

   ```bash
   ollama pull llama2
   ```

2. Clique em "üîÑ Reconectar ao Ollama" nas configura√ß√µes

### Transcri√ß√£o de √°udio n√£o funciona

1. **Para Whisper local**:

   - Verifique se `openai-whisper` est√° instalado: `pip install openai-whisper`
   - O primeiro uso pode demorar (baixa o modelo)

2. **Para OpenAI API**:
   - Verifique se `OPENAI_API_KEY` est√° configurada no `.env`
   - Verifique sua conex√£o com a internet

### Erro ao importar m√≥dulos

Certifique-se de que todas as depend√™ncias est√£o instaladas:

```bash
pip install -r requirements.txt
```

## Seguran√ßa

- **Nunca** commite o arquivo `.env` no controle de vers√£o
- Mantenha suas API Keys seguras e privadas
- O arquivo `.env` deve estar no `.gitignore` por padr√£o
- O Ollama roda localmente por padr√£o (sem exposi√ß√£o externa)

## Desenvolvimento

### Melhorias Futuras

- [ ] Suporte para streaming de respostas
- [ ] Exporta√ß√£o de conversas
- [ ] Temas personaliz√°veis
- [ ] Hist√≥rico persistente em banco de dados
- [ ] Suporte para m√∫ltiplos provedores de LLM
- [ ] Interface para gerenciar modelos Ollama

## Licen√ßa

Este projeto √© de c√≥digo aberto e est√° dispon√≠vel para uso educacional e pessoal.

## Contribuindo

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues ou pull requests.

## Suporte

Para problemas ou d√∫vidas:

1. Verifique se o Ollama est√° rodando e acess√≠vel
2. Confirme que todas as depend√™ncias est√£o instaladas
3. Verifique os logs de erro na interface
4. Consulte a documenta√ß√£o do Ollama: https://github.com/ollama/ollama

## Agradecimentos

- [Streamlit](https://streamlit.io/) pela excelente framework
- [Ollama](https://ollama.ai/) pela plataforma de modelos locais
- [OpenAI Whisper](https://github.com/openai/whisper) pela transcri√ß√£o de √°udio

---

Desenvolvido com Streamlit e Ollama
