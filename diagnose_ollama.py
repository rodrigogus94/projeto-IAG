"""
Script de diagnÃ³stico para verificar a conexÃ£o com o Ollama
Execute este script para identificar problemas de conexÃ£o
"""

import sys
import requests
from ollama_service import OllamaService
from llm_handler import create_llm_handler

def diagnose_ollama(base_url: str = "http://localhost:11434"):
    """
    Executa diagnÃ³stico completo da conexÃ£o com o Ollama.
    
    Args:
        base_url: URL base do servidor Ollama
    """
    print("=" * 60)
    print("ðŸ” DIAGNÃ“STICO DE CONEXÃƒO COM OLLAMA")
    print("=" * 60)
    print(f"\nðŸ“ URL configurada: {base_url}\n")
    
    # Teste 1: Verificar se a URL Ã© acessÃ­vel
    print("1ï¸âƒ£ Testando conectividade bÃ¡sica...")
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        print(f"   âœ… Resposta HTTP: {response.status_code}")
    except requests.exceptions.ConnectionError:
        print("   âŒ ERRO: NÃ£o foi possÃ­vel conectar ao Ollama")
        print("   ðŸ’¡ Verifique se o Ollama estÃ¡ rodando:")
        print("      - Windows: Verifique se o Ollama estÃ¡ instalado e rodando")
        print("      - Execute: ollama serve")
        print("      - Ou inicie o Ollama pelo menu iniciar")
        return False
    except requests.exceptions.Timeout:
        print("   âŒ ERRO: Timeout ao conectar")
        print("   ðŸ’¡ O servidor pode estar sobrecarregado ou inacessÃ­vel")
        return False
    except Exception as e:
        print(f"   âŒ ERRO: {str(e)}")
        return False
    
    # Teste 2: Verificar OllamaService
    print("\n2ï¸âƒ£ Testando OllamaService...")
    try:
        service = OllamaService(base_url=base_url)
        models = service.list_models()
        print(f"   âœ… OllamaService funcionando")
        print(f"   ðŸ“¦ Modelos encontrados: {len(models)}")
        if models:
            print("   Modelos disponÃ­veis:")
            for model in models[:5]:  # Mostrar apenas os 5 primeiros
                name = model.get("name", "Desconhecido")
                size = model.get("size", 0)
                size_gb = size / (1024**3) if size else 0
                print(f"      - {name} ({size_gb:.2f} GB)")
        else:
            print("   âš ï¸ Nenhum modelo encontrado")
            print("   ðŸ’¡ Baixe um modelo com: ollama pull llama2")
    except Exception as e:
        print(f"   âŒ ERRO no OllamaService: {str(e)}")
        return False
    
    # Teste 3: Verificar OllamaLLMHandler
    print("\n3ï¸âƒ£ Testando OllamaLLMHandler...")
    try:
        handler = create_llm_handler(base_url)
        if handler.is_configured():
            print("   âœ… Handler configurado corretamente")
            status = handler.get_connection_status()
            print(f"   ðŸ“Š Status: {status['message']}")
        else:
            print("   âŒ Handler nÃ£o estÃ¡ configurado")
            return False
    except Exception as e:
        print(f"   âŒ ERRO no Handler: {str(e)}")
        return False
    
    # Teste 4: Teste de geraÃ§Ã£o simples (opcional)
    print("\n4ï¸âƒ£ Testando geraÃ§Ã£o de resposta (opcional)...")
    try:
        handler = create_llm_handler(base_url)
        available_models = handler.list_available_models()
        if available_models:
            test_model = available_models[0]
            print(f"   ðŸ§ª Testando com modelo: {test_model}")
            response = handler.generate_response(
                user_input="OlÃ¡, vocÃª estÃ¡ funcionando?",
                model=test_model,
                temperature=0.7
            )
            if response and len(response) > 0:
                print(f"   âœ… Resposta recebida ({len(response)} caracteres)")
                print(f"   ðŸ“ Preview: {response[:100]}...")
            else:
                print("   âš ï¸ Resposta vazia recebida")
        else:
            print("   âš ï¸ Nenhum modelo disponÃ­vel para teste")
    except Exception as e:
        print(f"   âš ï¸ Erro no teste de geraÃ§Ã£o: {str(e)}")
        print("   (Isso pode ser normal se nÃ£o houver modelos instalados)")
    
    print("\n" + "=" * 60)
    print("âœ… DIAGNÃ“STICO CONCLUÃDO")
    print("=" * 60)
    return True

if __name__ == "__main__":
    # Permitir URL customizada via argumento
    url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:11434"
    diagnose_ollama(url)

