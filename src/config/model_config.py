"""
Configura√ß√µes centralizadas para o modelo de linguagem - OLLAMA
Este arquivo cont√©m todas as regras, par√¢metros e instru√ß√µes importantes para o modelo Ollama

NOTA: Este arquivo √© espec√≠fico para modelos rodando via Ollama.
Os par√¢metros e formatos s√£o compat√≠veis com a API do Ollama.
Para usar com outros provedores (OpenAI, Anthropic, etc.), ajuste os par√¢metros conforme necess√°rio.
"""

# ============================================================================
# SYSTEM PROMPT - Persona e Instru√ß√µes do Assistente
# ============================================================================

SYSTEM_PROMPT = """Voc√™ √© um assistente de IA inteligente e prestativo chamado Omnilink AI. 
Voc√™ ajuda usu√°rios a criar dashboards e visualiza√ß√µes de dados atrav√©s de conversas naturais.

REGRAS DE COMPORTAMENTO:
1. Seja sempre educado, profissional e prestativo
2. Responda em portugu√™s brasileiro, a menos que o usu√°rio solicite outro idioma
3. Seja conciso mas completo nas respostas
4. Se n√£o souber algo, admita honestamente
5. Mantenha o contexto da conversa anterior
6. Use formata√ß√£o Markdown para melhorar a legibilidade (t√≠tulos, listas, c√≥digo)
7. Quando apropriado, sugira melhorias ou alternativas

ESPECIALIDADES:
- An√°lise e visualiza√ß√£o de dados
- Cria√ß√£o de dashboards
- Explica√ß√£o de conceitos de forma clara
- Resolu√ß√£o de problemas t√©cnicos

FORMATO DE RESPOSTAS:
- Use t√≠tulos (##) para se√ß√µes importantes
- Use listas quando apropriado
- Use blocos de c√≥digo (```) para exemplos t√©cnicos
- Seja visual e estruturado
"""

# ============================================================================
# PAR√ÇMETROS PADR√ÉO DO MODELO
# ============================================================================

# Temperatura padr√£o (0.0 = determin√≠stico, 2.0 = muito criativo)
DEFAULT_TEMPERATURE = 0.7

# Modelo padr√£o (ser√° substitu√≠do pelo primeiro dispon√≠vel se n√£o existir)
DEFAULT_MODEL = "llama2:latest"

# Limites de temperatura
MIN_TEMPERATURE = 0.0
MAX_TEMPERATURE = 2.0

# Outros par√¢metros do Ollama (opcionais)
# Documenta√ß√£o: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter
DEFAULT_TOP_P = 0.9  # Nucleus sampling (0.0-1.0) - controla diversidade
DEFAULT_TOP_K = 40  # Top-k sampling - n√∫mero de tokens mais prov√°veis a considerar
DEFAULT_NUM_PREDICT = -1  # -1 = sem limite, ou n√∫mero m√°ximo de tokens a gerar
DEFAULT_REPEAT_PENALTY = 1.1  # Penalidade por repeti√ß√£o (1.0 = sem penalidade)
DEFAULT_SEED = -1  # Seed para reprodutibilidade (-1 = aleat√≥rio)

# ============================================================================
# REGRAS E RESTRI√á√ïES
# ============================================================================

MODEL_RULES = {
    "max_context_length": 4096,  # M√°ximo de tokens no contexto (ajustar conforme modelo)
    "max_response_length": 2048,  # M√°ximo de tokens na resposta
    "enable_streaming": False,  # Streaming de respostas (ser√° implementado)
    "timeout_seconds": 120,  # Timeout padr√£o para requisi√ß√µes (em segundos)
    # Pode ser sobrescrito por OLLAMA_TIMEOUT no .env
    # Para chat, o timeout √© automaticamente dobrado (240s)
}

# ============================================================================
# CONFIGURA√á√ïES DE COMPORTAMENTO
# ============================================================================

BEHAVIOR_CONFIG = {
    # Idioma padr√£o
    "default_language": "pt-BR",
    # Formato de resposta preferido
    "preferred_format": "markdown",
    # N√≠vel de detalhamento
    "detail_level": "balanced",  # "brief", "balanced", "detailed"
    # Incluir exemplos nas respostas
    "include_examples": True,
    # Sugerir melhorias automaticamente
    "suggest_improvements": True,
    # Admitir quando n√£o sabe algo
    "admit_uncertainty": True,
}

# ============================================================================
# PROMPTS ESPEC√çFICOS POR CONTEXTO
# ============================================================================

CONTEXT_PROMPTS = {
    "dashboard": """Quando o usu√°rio pedir para criar um dashboard:
1. Pergunte sobre os dados dispon√≠veis
2. Sugira tipos de visualiza√ß√£o apropriados
3. Explique as op√ß√µes de forma clara
4. Ofere√ßa exemplos pr√°ticos""",
    "data_analysis": """Quando o usu√°rio pedir an√°lise de dados:
1. Identifique o tipo de an√°lise necess√°ria
2. Sugira m√©todos apropriados
3. Explique os resultados de forma acess√≠vel
4. Ofere√ßa insights pr√°ticos""",
    "error_help": """Quando o usu√°rio reportar um erro:
1. Pe√ßa detalhes do erro
2. Sugira solu√ß√µes passo a passo
3. Explique o que pode ter causado
4. Ofere√ßa alternativas se necess√°rio""",
    "general": """Para conversas gerais:
1. Seja amig√°vel e prestativo
2. Mantenha o foco no objetivo do usu√°rio
3. Ofere√ßa ajuda adicional quando apropriado
4. Use linguagem clara e acess√≠vel""",
}

# ============================================================================
# MENSAGENS DO SISTEMA
# ============================================================================

SYSTEM_MESSAGES = {
    "welcome": "Ol√°! üëã Sou seu assistente de dashboards. Pe√ßa visualiza√ß√µes de dados e eu gero para voc√™ em tempo real!",
    "thinking": "üí≠ Pensando...",
    "error": "‚ùå Ocorreu um erro. Por favor, tente novamente.",
    "no_response": "N√£o foi poss√≠vel gerar uma resposta. Verifique sua conex√£o com o Ollama.",
    "model_not_found": "Modelo n√£o encontrado. Verifique se o modelo est√° instalado no Ollama.",
}

# ============================================================================
# VALIDA√á√ïES E LIMITES
# ============================================================================

VALIDATION_RULES = {
    "temperature_range": (MIN_TEMPERATURE, MAX_TEMPERATURE),
    "min_message_length": 1,
    "max_message_length": 10000,
    "allowed_languages": ["pt-BR", "en-US", "es-ES"],
}

# ============================================================================
# CONFIGURA√á√ïES AVAN√áADAS
# ============================================================================

ADVANCED_CONFIG = {
    # Retry em caso de falha
    "max_retries": 3,
    "retry_delay": 1.0,  # segundos
    # Cache de respostas (futuro)
    "enable_cache": False,
    "cache_ttl": 3600,  # segundos
    # Logging
    "log_requests": True,
    "log_responses": False,  # Pode conter dados sens√≠veis
    # Performance
    "enable_streaming": False,  # Ser√° implementado
    "stream_chunk_size": 50,  # Tokens por chunk no streaming
}

# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================


def get_system_prompt(context: str = "general") -> str:
    """
    Retorna o system prompt completo com contexto espec√≠fico.

    Args:
        context: Contexto da conversa ("dashboard", "data_analysis", "error_help", "general")

    Returns:
        System prompt completo
    """
    base_prompt = SYSTEM_PROMPT

    if context in CONTEXT_PROMPTS:
        context_instructions = CONTEXT_PROMPTS[context]
        return f"{base_prompt}\n\nCONTEXTO ATUAL:\n{context_instructions}"

    return base_prompt


def validate_temperature(temperature: float) -> float:
    """
    Valida e ajusta a temperatura para o range permitido.

    Args:
        temperature: Temperatura a validar

    Returns:
        Temperatura validada
    """
    min_temp, max_temp = VALIDATION_RULES["temperature_range"]
    return max(min_temp, min(max_temp, temperature))


def get_model_parameters(temperature: float = None, **kwargs) -> dict:
    """
    Retorna dicion√°rio com par√¢metros do modelo Ollama.

    Par√¢metros suportados pelo Ollama:
    - temperature: Controla aleatoriedade (0.0-2.0)
    - top_p: Nucleus sampling (0.0-1.0)
    - top_k: Top-k sampling (n√∫mero inteiro)
    - num_predict: M√°ximo de tokens a gerar (-1 = ilimitado)
    - repeat_penalty: Penalidade por repeti√ß√£o (1.0+)
    - seed: Seed para reprodutibilidade (-1 = aleat√≥rio)

    Args:
        temperature: Temperatura (usa padr√£o se None)
        **kwargs: Par√¢metros adicionais do Ollama

    Returns:
        Dicion√°rio com par√¢metros no formato esperado pelo Ollama
    """
    params = {
        "temperature": validate_temperature(temperature or DEFAULT_TEMPERATURE),
    }

    # Adicionar par√¢metros opcionais se fornecidos
    if "top_p" in kwargs:
        params["top_p"] = kwargs["top_p"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["top_p"] = DEFAULT_TOP_P

    if "top_k" in kwargs:
        params["top_k"] = kwargs["top_k"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["top_k"] = DEFAULT_TOP_K

    if "num_predict" in kwargs:
        params["num_predict"] = kwargs["num_predict"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["num_predict"] = DEFAULT_NUM_PREDICT

    if "repeat_penalty" in kwargs:
        params["repeat_penalty"] = kwargs["repeat_penalty"]

    if "seed" in kwargs:
        params["seed"] = kwargs["seed"]

    return params


def get_behavior_settings() -> dict:
    """
    Retorna configura√ß√µes de comportamento do modelo.

    Returns:
        Dicion√°rio com configura√ß√µes
    """
    return BEHAVIOR_CONFIG.copy()


def get_validation_rules() -> dict:
    """
    Retorna regras de valida√ß√£o.

    Returns:
        Dicion√°rio com regras
    """
    return VALIDATION_RULES.copy()

