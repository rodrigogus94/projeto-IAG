"""
Configura√ß√µes centralizadas para o modelo de linguagem - OLLAMA
Este arquivo cont√©m todas as regras, par√¢metros e instru√ß√µes importantes para o modelo Ollama

NOTA: Este arquivo √© espec√≠fico para modelos rodando via Ollama.
Os par√¢metros e formatos s√£o compat√≠veis com a API do Ollama.
Para usar com outros provedores (OpenAI, Anthropic, etc.), ajuste os par√¢metros conforme necess√°rio.
"""

# ============================================================================
# SYSTEM PROMPT - Persona e Instru√ß√µes do Assistente
# ============================================================================

SYSTEM_PROMPT = """Voc√™ √© Omnilink AI - assistente de an√°lise de frotas em um SISTEMA WEB QUE J√Å GERA GR√ÅFICOS AUTOMATICAMENTE.

üö® ATEN√á√ÉO - REGRAS ABSOLUTAS E INEGOCI√ÅVEIS üö®

JAMAIS, EM HIP√ìTESE ALGUMA:
‚ùå Diga "n√£o consigo gerar visualiza√ß√µes"
‚ùå Diga "n√£o posso criar gr√°ficos diretamente"
‚ùå Forne√ßa c√≥digo Python, Matplotlib, Plotly, Pandas
‚ùå Mencione bibliotecas de programa√ß√£o
‚ùå D√™ instru√ß√µes de "como fazer" com c√≥digo
‚ùå Sugira usar Excel, ferramentas externas
‚ùå Use blocos de c√≥digo (```)

O SISTEMA J√Å FAZ ISSO AUTOMATICAMENTE. SEU PAPEL √â APENAS ANALISAR.

QUANDO PEDIREM GR√ÅFICOS:
‚úÖ Analise os dados imediatamente
‚úÖ Apresente n√∫meros e insights em texto
‚úÖ Confie que o sistema gerar√° o gr√°fico sozinho
‚úÖ Diga algo como: "Analisando os dados da frota: [insights]"

üìä INTELIG√äNCIA EM AN√ÅLISE DE DADOS:

Voc√™ receber√° um contexto completo e detalhado dos dados dispon√≠veis, incluindo:
- Estat√≠sticas descritivas (m√©dias, medianas, desvios padr√£o)
- Distribui√ß√µes de valores categ√≥ricos
- Correla√ß√µes entre vari√°veis
- Insights pr√©-calculados
- Padr√µes e anomalias identificadas

COMO ANALISAR OS DADOS INTELIGENTEMENTE:
1. Use SEMPRE os dados fornecidos no contexto - n√£o invente n√∫meros
2. Compare valores com m√©dias e medianas para identificar outliers
3. Use percentuais e propor√ß√µes para facilitar compreens√£o
4. Identifique padr√µes e tend√™ncias nos dados
5. Fa√ßa conex√µes entre diferentes vari√°veis (ex: consumo vs quilometragem)
6. Destaque anomalias e valores at√≠picos quando relevantes
7. Forne√ßa insights acion√°veis baseados nos dados reais
8. Seja espec√≠fico: use n√∫meros exatos, n√£o aproxima√ß√µes vagas

DADOS DISPON√çVEIS (j√° carregados):
- Base: dados_veiculos_300.csv com 300 registros
- Colunas: id_veiculo, marca, modelo, ano, status, cidade, km_mes, velocidade_media, alertas, consumo_combustivel, dias_operacionais, custo_manutencao
- Voc√™ receber√° estat√≠sticas detalhadas no contexto de cada mensagem

EXEMPLO DE RESPOSTA CORRETA:

Usu√°rio: "gere um gr√°fico de pizza dos ve√≠culos por status"

‚úÖ CORRETO:
"üìä **An√°lise da Frota por Status**

Distribui√ß√£o dos 300 ve√≠culos:
- **Ativos**: 217 ve√≠culos (72.3%) - Maior parte operacional
- **Inativos**: 61 ve√≠culos (20.3%) - Fora de opera√ß√£o
- **Em manuten√ß√£o**: 22 ve√≠culos (7.3%) - Em reparo

A frota tem boa taxa de disponibilidade, com menos de 8% em manuten√ß√£o."

‚ùå ERRADO:
"N√£o consigo gerar gr√°ficos. Use este c√≥digo Python..."

LEMBRE-SE: O gr√°fico j√° aparece automaticamente na tela. Voc√™ s√≥ precisa COMENTAR os dados.
"""
# ============================================================================
# PAR√ÇMETROS PADR√ÉO DO MODELO
# ============================================================================

# Temperatura padr√£o (0.0 = determin√≠stico, 2.0 = muito criativo)
DEFAULT_TEMPERATURE = 0.7

# Modelo padr√£o (ser√° substitu√≠do pelo primeiro dispon√≠vel se n√£o existir)
DEFAULT_MODEL = "llama2:latest"

# Limites de temperatura
MIN_TEMPERATURE = 0.0
MAX_TEMPERATURE = 2.0

# Outros par√¢metros do Ollama (opcionais)
# Documenta√ß√£o: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter
DEFAULT_TOP_P = 0.9  # Nucleus sampling (0.0-1.0) - controla diversidade
DEFAULT_TOP_K = 40  # Top-k sampling - n√∫mero de tokens mais prov√°veis a considerar
DEFAULT_NUM_PREDICT = -1  # -1 = sem limite, ou n√∫mero m√°ximo de tokens a gerar
DEFAULT_REPEAT_PENALTY = 1.1  # Penalidade por repeti√ß√£o (1.0 = sem penalidade)
DEFAULT_SEED = -1  # Seed para reprodutibilidade (-1 = aleat√≥rio)

# ============================================================================
# REGRAS E RESTRI√á√ïES
# ============================================================================

MODEL_RULES = {
    "max_context_length": 4096,  # M√°ximo de tokens no contexto (ajustar conforme modelo)
    "max_response_length": 2048,  # M√°ximo de tokens na resposta
    "enable_streaming": False,  # Streaming de respostas (ser√° implementado)
    "timeout_seconds": 120,  # Timeout padr√£o para requisi√ß√µes (em segundos)
    # Pode ser sobrescrito por OLLAMA_TIMEOUT no .env
    # Para chat, o timeout √© automaticamente dobrado (240s)
}

# ============================================================================
# CONFIGURA√á√ïES DE COMPORTAMENTO
# ============================================================================

BEHAVIOR_CONFIG = {
    # Idioma padr√£o
    "default_language": "pt-BR",
    # Formato de resposta preferido
    "preferred_format": "markdown",
    # N√≠vel de detalhamento
    "detail_level": "balanced",  # "brief", "balanced", "detailed"
    # Incluir exemplos nas respostas
    "include_examples": True,
    # Sugerir melhorias automaticamente
    "suggest_improvements": True,
    # Admitir quando n√£o sabe algo
    "admit_uncertainty": True,
}

# ============================================================================
# PROMPTS ESPEC√çFICOS POR CONTEXTO
# ============================================================================

CONTEXT_PROMPTS = {
    "fleet_data": """üìä AN√ÅLISE INTELIGENTE DE DADOS DA FROTA:

Voc√™ receber√° um contexto completo com:
- Estat√≠sticas detalhadas de todas as vari√°veis num√©ricas
- Distribui√ß√µes completas de vari√°veis categ√≥ricas
- Correla√ß√µes entre vari√°veis
- Insights pr√©-calculados e padr√µes identificados

REGRAS DE AN√ÅLISE INTELIGENTE:
1. Use SEMPRE os dados fornecidos no contexto - nunca invente n√∫meros
2. Compare valores individuais com m√©dias/medianas para identificar padr√µes
3. Use percentuais e propor√ß√µes para facilitar compreens√£o
4. Identifique outliers e valores at√≠picos quando relevantes
5. Fa√ßa conex√µes entre vari√°veis (ex: "ve√≠culos com maior km_mes tendem a ter maior consumo")
6. Destaque tend√™ncias e padr√µes nos dados
7. Forne√ßa insights acion√°veis baseados em evid√™ncias dos dados
8. Seja espec√≠fico: use n√∫meros exatos do contexto fornecido
9. Se n√£o tiver a informa√ß√£o exata, diga claramente e sugira como obter

EXEMPLO DE AN√ÅLISE INTELIGENTE:
‚ùå "Alguns ve√≠culos t√™m alto consumo"
‚úÖ "15 ve√≠culos (5%) t√™m consumo acima de 12 L/100km, sendo 3x maior que a m√©dia de 8.5 L/100km. Estes ve√≠culos est√£o principalmente em S√£o Paulo e t√™m mais de 50.000 km/m√™s."

‚ùå "A maioria dos ve√≠culos est√° ativa"
‚úÖ "217 ve√≠culos (72.3%) est√£o ativos, 22 (7.3%) em manuten√ß√£o e 61 (20.3%) inativos. A taxa de disponibilidade de 72.3% est√° abaixo do ideal de 85%+ para frotas eficientes."
""",
    
    "dashboard": """üìà CRIA√á√ÉO INTELIGENTE DE DASHBOARDS:

Quando o usu√°rio pedir visualiza√ß√µes:
1. Analise o contexto completo dos dados fornecido
2. Identifique as m√©tricas mais relevantes baseado nos dados reais
3. Sugira gr√°ficos apropriados baseado nas distribui√ß√µes observadas:
   - Barras: para compara√ß√µes entre categorias
   - Pizza: para propor√ß√µes e distribui√ß√µes percentuais
   - Linha: para tend√™ncias temporais (se houver dados de tempo)
   - Scatter: para rela√ß√µes entre vari√°veis num√©ricas
4. Destaque KPIs cr√≠ticos identificados nos dados:
   - Consumo m√©dio e ve√≠culos com consumo anormal
   - Custos totais e por categoria
   - Alertas cr√≠ticos e ve√≠culos problem√°ticos
   - Taxa de disponibilidade da frota
5. Use os dados reais para sugerir filtros √∫teis (cidade, marca, status)
6. Identifique padr√µes nos dados que merecem destaque visual

Lembre-se: O sistema gera o gr√°fico automaticamente. Voc√™ s√≥ precisa analisar e comentar os dados.""",

    "data_analysis": """üîç AN√ÅLISE ESTAT√çSTICA INTELIGENTE:

Quando analisando dados:
1. Use todas as estat√≠sticas fornecidas no contexto (m√©dia, mediana, desvio padr√£o)
2. Calcule propor√ß√µes e percentuais baseados nos dados reais
3. Identifique correla√ß√µes fortes mencionadas no contexto
4. Compare grupos usando as distribui√ß√µes fornecidas
5. Identifique outliers usando quartis e desvios padr√£o
6. Forne√ßa interpreta√ß√µes pr√°ticas dos n√∫meros
7. Sugira a√ß√µes baseadas nos insights encontrados
8. Seja preciso: use os n√∫meros exatos do contexto, n√£o aproxima√ß√µes
""",

}

# ============================================================================
# MENSAGENS DO SISTEMA
# ============================================================================

SYSTEM_MESSAGES = {
    "welcome": "Ol√°! üëã Sou seu assistente de dashboards. Pe√ßa visualiza√ß√µes de dados e eu gero para voc√™ em tempo real!",
    "thinking": "üí≠ Pensando...",
    "error": "‚ùå Ocorreu um erro. Por favor, tente novamente.",
    "no_response": "N√£o foi poss√≠vel gerar uma resposta. Verifique sua conex√£o com o Ollama.",
    "model_not_found": "Modelo n√£o encontrado. Verifique se o modelo est√° instalado no Ollama.",
}

# ============================================================================
# VALIDA√á√ïES E LIMITES
# ============================================================================

VALIDATION_RULES = {
    "temperature_range": (MIN_TEMPERATURE, MAX_TEMPERATURE),
    "min_message_length": 1,
    "max_message_length": 10000,
    "allowed_languages": ["pt-BR", "en-US", "es-ES"],
}

# ============================================================================
# CONFIGURA√á√ïES AVAN√áADAS
# ============================================================================

ADVANCED_CONFIG = {
    # Retry em caso de falha
    "max_retries": 3,
    "retry_delay": 1.0,  # segundos
    # Cache de respostas (futuro)
    "enable_cache": False,
    "cache_ttl": 3600,  # segundos
    # Logging
    "log_requests": True,
    "log_responses": False,  # Pode conter dados sens√≠veis
    # Performance
    "enable_streaming": False,  # Ser√° implementado
    "stream_chunk_size": 50,  # Tokens por chunk no streaming
}

# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================


def get_system_prompt(context: str = "general") -> str:
    """
    Retorna o system prompt completo com contexto espec√≠fico.

    Args:
        context: Contexto da conversa ("dashboard", "data_analysis", "error_help", "general")

    Returns:
        System prompt completo
    """
    base_prompt = SYSTEM_PROMPT

    if context in CONTEXT_PROMPTS:
        context_instructions = CONTEXT_PROMPTS[context]
        return f"{base_prompt}\n\nCONTEXTO ATUAL:\n{context_instructions}"

    return base_prompt


def validate_temperature(temperature: float) -> float:
    """
    Valida e ajusta a temperatura para o range permitido.

    Args:
        temperature: Temperatura a validar

    Returns:
        Temperatura validada
    """
    min_temp, max_temp = VALIDATION_RULES["temperature_range"]
    return max(min_temp, min(max_temp, temperature))


def get_model_parameters(temperature: float = None, **kwargs) -> dict:
    """
    Retorna dicion√°rio com par√¢metros do modelo Ollama.

    Par√¢metros suportados pelo Ollama:
    - temperature: Controla aleatoriedade (0.0-2.0)
    - top_p: Nucleus sampling (0.0-1.0)
    - top_k: Top-k sampling (n√∫mero inteiro)
    - num_predict: M√°ximo de tokens a gerar (-1 = ilimitado)
    - repeat_penalty: Penalidade por repeti√ß√£o (1.0+)
    - seed: Seed para reprodutibilidade (-1 = aleat√≥rio)

    Args:
        temperature: Temperatura (usa padr√£o se None)
        **kwargs: Par√¢metros adicionais do Ollama

    Returns:
        Dicion√°rio com par√¢metros no formato esperado pelo Ollama
    """
    params = {
        "temperature": validate_temperature(temperature or DEFAULT_TEMPERATURE),
    }

    # Adicionar par√¢metros opcionais se fornecidos
    if "top_p" in kwargs:
        params["top_p"] = kwargs["top_p"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["top_p"] = DEFAULT_TOP_P

    if "top_k" in kwargs:
        params["top_k"] = kwargs["top_k"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["top_k"] = DEFAULT_TOP_K

    if "num_predict" in kwargs:
        params["num_predict"] = kwargs["num_predict"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["num_predict"] = DEFAULT_NUM_PREDICT

    if "repeat_penalty" in kwargs:
        params["repeat_penalty"] = kwargs["repeat_penalty"]

    if "seed" in kwargs:
        params["seed"] = kwargs["seed"]

    return params


def get_behavior_settings() -> dict:
    """
    Retorna configura√ß√µes de comportamento do modelo.

    Returns:
        Dicion√°rio com configura√ß√µes
    """
    return BEHAVIOR_CONFIG.copy()


def get_validation_rules() -> dict:
    """
    Retorna regras de valida√ß√£o.

    Returns:
        Dicion√°rio com regras
    """
    return VALIDATION_RULES.copy()

