"""
Configura√ß√µes centralizadas para o modelo de linguagem - OLLAMA
Este arquivo cont√©m todas as regras, par√¢metros e instru√ß√µes importantes para o modelo Ollama

NOTA: Este arquivo √© espec√≠fico para modelos rodando via Ollama.
Os par√¢metros e formatos s√£o compat√≠veis com a API do Ollama.
Para usar com outros provedores (OpenAI, Anthropic, etc.), ajuste os par√¢metros conforme necess√°rio.
"""

# ============================================================================
# SYSTEM PROMPT - Persona e Instru√ß√µes do Assistente
# ============================================================================

SYSTEM_PROMPT = """Voc√™ √© Omnilink AI - assistente de an√°lise de frotas em um SISTEMA WEB QUE J√Å GERA GR√ÅFICOS AUTOMATICAMENTE.

üö® ATEN√á√ÉO - REGRAS ABSOLUTAS E INEGOCI√ÅVEIS üö®

JAMAIS, EM HIP√ìTESE ALGUMA:
‚ùå Diga "n√£o consigo gerar visualiza√ß√µes"
‚ùå Diga "n√£o posso criar gr√°ficos diretamente"
‚ùå Forne√ßa c√≥digo Python, Matplotlib, Plotly, Pandas
‚ùå Mencione bibliotecas de programa√ß√£o
‚ùå D√™ instru√ß√µes de "como fazer" com c√≥digo
‚ùå Sugira usar Excel, ferramentas externas
‚ùå Use blocos de c√≥digo (```)

O SISTEMA J√Å FAZ ISSO AUTOMATICAMENTE. SEU PAPEL √â APENAS ANALISAR.

QUANDO PEDIREM GR√ÅFICOS:
‚úÖ Analise os dados imediatamente
‚úÖ Apresente n√∫meros e insights em texto
‚úÖ Confie que o sistema gerar√° o gr√°fico sozinho
‚úÖ Diga algo como: "Analisando os dados da frota: [insights]"

DADOS DISPON√çVEIS (j√° carregados):
- Base: dados_veiculos_300.csv com 300 registros
- Colunas: id_veiculo, marca, modelo, ano, status, cidade, km_mes, velocidade_media, alertas, consumo_combustivel, dias_operacionais, custo_manutencao

EXEMPLO DE RESPOSTA CORRETA:

Usu√°rio: "gere um gr√°fico de pizza dos ve√≠culos por status"

‚úÖ CORRETO:
"üìä **An√°lise da Frota por Status**

Distribui√ß√£o dos 300 ve√≠culos:
- **Ativos**: 217 ve√≠culos (72.3%) - Maior parte operacional
- **Inativos**: 61 ve√≠culos (20.3%) - Fora de opera√ß√£o
- **Em manuten√ß√£o**: 22 ve√≠culos (7.3%) - Em reparo

A frota tem boa taxa de disponibilidade, com menos de 8% em manuten√ß√£o."

‚ùå ERRADO:
"N√£o consigo gerar gr√°ficos. Use este c√≥digo Python..."

LEMBRE-SE: O gr√°fico j√° aparece automaticamente na tela. Voc√™ s√≥ precisa COMENTAR os dados.
"""
# ============================================================================
# PAR√ÇMETROS PADR√ÉO DO MODELO
# ============================================================================

# Temperatura padr√£o (0.0 = determin√≠stico, 2.0 = muito criativo)
DEFAULT_TEMPERATURE = 0.7

# Modelo padr√£o (ser√° substitu√≠do pelo primeiro dispon√≠vel se n√£o existir)
DEFAULT_MODEL = "llama2:latest"

# Limites de temperatura
MIN_TEMPERATURE = 0.0
MAX_TEMPERATURE = 2.0

# Outros par√¢metros do Ollama (opcionais)
# Documenta√ß√£o: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter
DEFAULT_TOP_P = 0.9  # Nucleus sampling (0.0-1.0) - controla diversidade
DEFAULT_TOP_K = 40  # Top-k sampling - n√∫mero de tokens mais prov√°veis a considerar
DEFAULT_NUM_PREDICT = -1  # -1 = sem limite, ou n√∫mero m√°ximo de tokens a gerar
DEFAULT_REPEAT_PENALTY = 1.1  # Penalidade por repeti√ß√£o (1.0 = sem penalidade)
DEFAULT_SEED = -1  # Seed para reprodutibilidade (-1 = aleat√≥rio)

# ============================================================================
# REGRAS E RESTRI√á√ïES
# ============================================================================

MODEL_RULES = {
    "max_context_length": 4096,  # M√°ximo de tokens no contexto (ajustar conforme modelo)
    "max_response_length": 2048,  # M√°ximo de tokens na resposta
    "enable_streaming": False,  # Streaming de respostas (ser√° implementado)
    "timeout_seconds": 120,  # Timeout padr√£o para requisi√ß√µes (em segundos)
    # Pode ser sobrescrito por OLLAMA_TIMEOUT no .env
    # Para chat, o timeout √© automaticamente dobrado (240s)
}

# ============================================================================
# CONFIGURA√á√ïES DE COMPORTAMENTO
# ============================================================================

BEHAVIOR_CONFIG = {
    # Idioma padr√£o
    "default_language": "pt-BR",
    # Formato de resposta preferido
    "preferred_format": "markdown",
    # N√≠vel de detalhamento
    "detail_level": "balanced",  # "brief", "balanced", "detailed"
    # Incluir exemplos nas respostas
    "include_examples": True,
    # Sugerir melhorias automaticamente
    "suggest_improvements": True,
    # Admitir quando n√£o sabe algo
    "admit_uncertainty": True,
}

# ============================================================================
# PROMPTS ESPEC√çFICOS POR CONTEXTO
# ============================================================================

CONTEXT_PROMPTS = {
<<<<<<< HEAD
    "dashboard": """Quando o usu√°rio pedir para criar um dashboard:
1. Pergunte sobre os dados dispon√≠veis
2. Sugira tipos de visualiza√ß√£o apropriados
3. Explique as op√ß√µes de forma clara
4. Ofere√ßa exemplos pr√°ticos""",
    "data_analysis": """Quando o usu√°rio pedir an√°lise de dados:
1. Identifique o tipo de an√°lise necess√°ria
2. Sugira m√©todos apropriados
3. Explique os resultados de forma acess√≠vel
4. Ofere√ßa insights pr√°ticos""",
    "error_help": """Quando o usu√°rio reportar um erro:
1. Pe√ßa detalhes do erro
2. Sugira solu√ß√µes passo a passo
3. Explique o que pode ter causado
4. Ofere√ßa alternativas se necess√°rio""",
    "general": """Para conversas gerais:
1. Seja amig√°vel e prestativo
2. Mantenha o foco no objetivo do usu√°rio
3. Ofere√ßa ajuda adicional quando apropriado
4. Use linguagem clara e acess√≠vel""",
=======
    "fleet_data": """DADOS DA FROTA DISPON√çVEIS:
{data_summary}

REGRAS IMPORTANTES:
1. Use APENAS os dados acima para responder
2. N√£o invente informa√ß√µes que n√£o estejam na base
3. Para gr√°ficos, baseie-se nas colunas dispon√≠veis
4. Seja espec√≠fico sobre o que os dados mostram
5. Se n√£o tiver a informa√ß√£o, diga claramente""",
    
    "dashboard": """Quando o usu√°rio pedir para criar um dashboard:
1. Identifique quais m√©tricas da frota s√£o relevantes
2. Sugira gr√°ficos apropriados (barras para compara√ß√µes, pizza para distribui√ß√£o, linha para tend√™ncias)
3. Destaque KPIs importantes (consumo m√©dio, custos, alertas cr√≠ticos)
4. Ofere√ßa filtros por cidade, marca, status""",
    
    # ... resto dos contextos existentes
>>>>>>> 9ff461a1e44d6fbdeb3e94597c4e3346c0321e91
}

# ============================================================================
# MENSAGENS DO SISTEMA
# ============================================================================

SYSTEM_MESSAGES = {
    "welcome": "Ol√°! üëã Sou seu assistente de dashboards. Pe√ßa visualiza√ß√µes de dados e eu gero para voc√™ em tempo real!",
    "thinking": "üí≠ Pensando...",
    "error": "‚ùå Ocorreu um erro. Por favor, tente novamente.",
    "no_response": "N√£o foi poss√≠vel gerar uma resposta. Verifique sua conex√£o com o Ollama.",
    "model_not_found": "Modelo n√£o encontrado. Verifique se o modelo est√° instalado no Ollama.",
}

# ============================================================================
# VALIDA√á√ïES E LIMITES
# ============================================================================

VALIDATION_RULES = {
    "temperature_range": (MIN_TEMPERATURE, MAX_TEMPERATURE),
    "min_message_length": 1,
    "max_message_length": 10000,
    "allowed_languages": ["pt-BR", "en-US", "es-ES"],
}

# ============================================================================
# CONFIGURA√á√ïES AVAN√áADAS
# ============================================================================

ADVANCED_CONFIG = {
    # Retry em caso de falha
    "max_retries": 3,
    "retry_delay": 1.0,  # segundos
    # Cache de respostas (futuro)
    "enable_cache": False,
    "cache_ttl": 3600,  # segundos
    # Logging
    "log_requests": True,
    "log_responses": False,  # Pode conter dados sens√≠veis
    # Performance
    "enable_streaming": False,  # Ser√° implementado
    "stream_chunk_size": 50,  # Tokens por chunk no streaming
}

# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================


def get_system_prompt(context: str = "general") -> str:
    """
    Retorna o system prompt completo com contexto espec√≠fico.

    Args:
        context: Contexto da conversa ("dashboard", "data_analysis", "error_help", "general")

    Returns:
        System prompt completo
    """
    base_prompt = SYSTEM_PROMPT

    if context in CONTEXT_PROMPTS:
        context_instructions = CONTEXT_PROMPTS[context]
        return f"{base_prompt}\n\nCONTEXTO ATUAL:\n{context_instructions}"

    return base_prompt


def validate_temperature(temperature: float) -> float:
    """
    Valida e ajusta a temperatura para o range permitido.

    Args:
        temperature: Temperatura a validar

    Returns:
        Temperatura validada
    """
    min_temp, max_temp = VALIDATION_RULES["temperature_range"]
    return max(min_temp, min(max_temp, temperature))


def get_model_parameters(temperature: float = None, **kwargs) -> dict:
    """
    Retorna dicion√°rio com par√¢metros do modelo Ollama.

    Par√¢metros suportados pelo Ollama:
    - temperature: Controla aleatoriedade (0.0-2.0)
    - top_p: Nucleus sampling (0.0-1.0)
    - top_k: Top-k sampling (n√∫mero inteiro)
    - num_predict: M√°ximo de tokens a gerar (-1 = ilimitado)
    - repeat_penalty: Penalidade por repeti√ß√£o (1.0+)
    - seed: Seed para reprodutibilidade (-1 = aleat√≥rio)

    Args:
        temperature: Temperatura (usa padr√£o se None)
        **kwargs: Par√¢metros adicionais do Ollama

    Returns:
        Dicion√°rio com par√¢metros no formato esperado pelo Ollama
    """
    params = {
        "temperature": validate_temperature(temperature or DEFAULT_TEMPERATURE),
    }

    # Adicionar par√¢metros opcionais se fornecidos
    if "top_p" in kwargs:
        params["top_p"] = kwargs["top_p"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["top_p"] = DEFAULT_TOP_P

    if "top_k" in kwargs:
        params["top_k"] = kwargs["top_k"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["top_k"] = DEFAULT_TOP_K

    if "num_predict" in kwargs:
        params["num_predict"] = kwargs["num_predict"]
    elif "use_defaults" not in kwargs or kwargs.get("use_defaults"):
        params["num_predict"] = DEFAULT_NUM_PREDICT

    if "repeat_penalty" in kwargs:
        params["repeat_penalty"] = kwargs["repeat_penalty"]

    if "seed" in kwargs:
        params["seed"] = kwargs["seed"]

    return params


def get_behavior_settings() -> dict:
    """
    Retorna configura√ß√µes de comportamento do modelo.

    Returns:
        Dicion√°rio com configura√ß√µes
    """
    return BEHAVIOR_CONFIG.copy()


def get_validation_rules() -> dict:
    """
    Retorna regras de valida√ß√£o.

    Returns:
        Dicion√°rio com regras
    """
    return VALIDATION_RULES.copy()

