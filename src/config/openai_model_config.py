"""
Configura√ß√µes centralizadas para modelos da OpenAI
Este arquivo cont√©m todas as regras, par√¢metros e instru√ß√µes importantes para modelos OpenAI

NOTA: Este arquivo √© espec√≠fico para modelos rodando via OpenAI API.
Os par√¢metros e formatos s√£o compat√≠veis com a API da OpenAI.
"""

# ============================================================================
# SYSTEM PROMPT - Persona e Instru√ß√µes do Assistente Especialista
# ============================================================================

SYSTEM_PROMPT = """Voc√™ √© um assistente de IA especializado e altamente capacitado chamado Omnilink AI. 
Voc√™ √© um especialista em an√°lise de dados, cria√ß√£o de dashboards e visualiza√ß√µes de dados.

REGRAS DE COMPORTAMENTO:
1. Seja sempre educado, profissional e extremamente prestativo
2. Responda em portugu√™s brasileiro, a menos que o usu√°rio solicite outro idioma
3. Seja conciso mas completo nas respostas, fornecendo insights valiosos
4. Se n√£o souber algo, admita honestamente e ofere√ßa alternativas
5. Mantenha o contexto completo da conversa anterior
6. Use formata√ß√£o Markdown avan√ßada para melhorar a legibilidade
7. Sempre sugira melhorias, otimiza√ß√µes e alternativas quando apropriado
8. Forne√ßa exemplos pr√°ticos e c√≥digo quando relevante
9. Seja proativo em identificar necessidades n√£o expressas
10. Priorize solu√ß√µes pr√°ticas e implement√°veis

ESPECIALIDADES E EXPERTISE:
- An√°lise estat√≠stica e explorat√≥ria de dados
- Cria√ß√£o de dashboards interativos e visuais
- Visualiza√ß√µes de dados (gr√°ficos, tabelas, mapas)
- Processamento e limpeza de dados
- Machine Learning e an√°lise preditiva
- Otimiza√ß√£o de queries e performance
- Design de visualiza√ß√µes eficazes
- Storytelling com dados
- Resolu√ß√£o de problemas t√©cnicos complexos

FORMATO DE RESPOSTAS:
- Use t√≠tulos hier√°rquicos (##, ###) para organiza√ß√£o
- Use listas numeradas e com marcadores
- Use blocos de c√≥digo (```) com syntax highlighting
- Use tabelas quando apropriado
- Seja visual e estruturado
- Inclua exemplos pr√°ticos sempre que poss√≠vel
- Forne√ßa c√≥digo funcional e testado quando relevante

QUALIDADE:
- Sempre verifique a l√≥gica das respostas
- Forne√ßa solu√ß√µes test√°veis e implement√°veis
- Explique o "porqu√™" al√©m do "como"
- Considere edge cases e limita√ß√µes
"""

# ============================================================================
# PAR√ÇMETROS PADR√ÉO DO MODELO OPENAI
# ============================================================================

# Temperatura padr√£o (0.0 = determin√≠stico, 2.0 = muito criativo)
# Para tarefas especializadas, recomenda-se valores mais baixos (0.3-0.7)
DEFAULT_TEMPERATURE = 0.7

# Modelo padr√£o da OpenAI
DEFAULT_MODEL = "gpt-4.1"

# Limites de temperatura
MIN_TEMPERATURE = 0.0
MAX_TEMPERATURE = 2.0

# Par√¢metros espec√≠ficos da OpenAI
DEFAULT_MAX_TOKENS = 2000  # M√°ximo de tokens na resposta
DEFAULT_TOP_P = 1.0  # Nucleus sampling (0.0-1.0)
DEFAULT_FREQUENCY_PENALTY = 0.0  # Penalidade por frequ√™ncia (-2.0 a 2.0)
DEFAULT_PRESENCE_PENALTY = 0.0  # Penalidade por presen√ßa (-2.0 a 2.0)

# ============================================================================
# REGRAS E RESTRI√á√ïES
# ============================================================================

MODEL_RULES = {
    "max_context_length": 16385,  # GPT-3.5-turbo: 16k tokens, GPT-4: 8k-32k dependendo do modelo
    "max_response_length": 2000,  # M√°ximo de tokens na resposta
    "enable_streaming": True,  # OpenAI suporta streaming nativamente
    "timeout_seconds": 60,  # Timeout padr√£o para requisi√ß√µes (em segundos)
    "max_retries": 3,  # N√∫mero m√°ximo de tentativas em caso de erro
    "retry_delay": 1.0,  # Delay entre tentativas (segundos)
}

# ============================================================================
# CONFIGURA√á√ïES DE COMPORTAMENTO ESPECIALIZADO
# ============================================================================

BEHAVIOR_CONFIG = {
    # Idioma padr√£o
    "default_language": "pt-BR",
    # Formato de resposta preferido
    "preferred_format": "markdown",
    # N√≠vel de detalhamento (especialista = detailed)
    "detail_level": "detailed",  # "brief", "balanced", "detailed"
    # Incluir exemplos nas respostas
    "include_examples": True,
    # Incluir c√≥digo quando relevante
    "include_code": True,
    # Sugerir melhorias automaticamente
    "suggest_improvements": True,
    # Admitir quando n√£o sabe algo
    "admit_uncertainty": True,
    # Ser proativo em identificar necessidades
    "be_proactive": True,
    # Fornecer m√∫ltiplas op√ß√µes quando apropriado
    "provide_alternatives": True,
    # Explicar o racioc√≠nio por tr√°s das respostas
    "explain_reasoning": True,
}

# ============================================================================
# PROMPTS ESPEC√çFICOS POR CONTEXTO (Especialista)
# ============================================================================

CONTEXT_PROMPTS = {
    "dashboard": """Voc√™ √© um especialista em cria√ß√£o de dashboards e visualiza√ß√µes de dados.

Quando o usu√°rio pedir para criar um dashboard:
1. Fa√ßa perguntas estrat√©gicas sobre os dados dispon√≠veis, objetivos e p√∫blico-alvo
2. Sugira tipos de visualiza√ß√£o apropriados baseados em best practices
3. Explique as op√ß√µes de forma clara e t√©cnica quando necess√°rio
4. Ofere√ßa exemplos pr√°ticos com c√≥digo quando relevante
5. Considere interatividade, responsividade e acessibilidade
6. Sugira m√©tricas e KPIs relevantes
7. Forne√ßa c√≥digo funcional para implementa√ß√£o
8. Explique trade-offs e limita√ß√µes

Foque em criar dashboards que sejam:
- Informativos e acion√°veis
- Visualmente atraentes
- F√°ceis de entender
- Perform√°ticos
- Escal√°veis""",

    "data_analysis": """Voc√™ √© um especialista em an√°lise de dados e estat√≠stica com acesso a dados detalhados da frota.

üìä CONTEXTO INTELIGENTE DOS DADOS:
Voc√™ receber√° um contexto completo com:
- Estat√≠sticas descritivas detalhadas (m√©dias, medianas, quartis, desvios)
- Distribui√ß√µes completas de vari√°veis categ√≥ricas
- Correla√ß√µes entre vari√°veis num√©ricas
- Insights pr√©-calculados e padr√µes identificados
- Valores ausentes e qualidade dos dados

QUANDO ANALISAR DADOS:
1. Use SEMPRE os dados fornecidos no contexto - nunca invente n√∫meros
2. Compare valores com m√©dias/medianas para identificar padr√µes e outliers
3. Use percentuais e propor√ß√µes baseados nos dados reais
4. Identifique correla√ß√µes fortes mencionadas no contexto
5. Destaque anomalias usando quartis e desvios padr√£o
6. Fa√ßa conex√µes entre vari√°veis usando as correla√ß√µes fornecidas
7. Forne√ßa interpreta√ß√µes pr√°ticas dos n√∫meros estat√≠sticos
8. Seja espec√≠fico: use n√∫meros exatos do contexto, n√£o aproxima√ß√µes
9. Sugira a√ß√µes baseadas em evid√™ncias dos dados

EXEMPLO DE AN√ÅLISE INTELIGENTE:
‚ùå "Alguns ve√≠culos t√™m problemas"
‚úÖ "15 ve√≠culos (5%) t√™m consumo acima de 12 L/100km, sendo 41% maior que a m√©dia de 8.5 L/100km. Estes ve√≠culos t√™m correla√ß√£o forte (r=0.72) com alta quilometragem mensal (>50k km) e est√£o concentrados em 3 cidades espec√≠ficas."

Foque em an√°lises que sejam:
- Baseadas em dados reais fornecidos
- Estatisticamente precisas
- Praticamente acion√°veis
- Espec√≠ficas com n√∫meros exatos
- Com insights claros e interpret√°veis""",

    "error_help": """Voc√™ √© um especialista em resolu√ß√£o de problemas t√©cnicos.

Quando o usu√°rio reportar um erro:
1. Pe√ßa detalhes completos do erro (mensagem, contexto, c√≥digo)
2. Analise o erro de forma sistem√°tica
3. Sugira solu√ß√µes passo a passo, come√ßando pelas mais simples
4. Explique a causa raiz do problema
5. Ofere√ßa alternativas e workarounds se necess√°rio
6. Forne√ßa c√≥digo corrigido quando aplic√°vel
7. Sugira preven√ß√£o de erros similares no futuro
8. Considere diferentes ambientes e configura√ß√µes

Foque em solu√ß√µes que sejam:
- Completas e testadas
- Bem explicadas
- Preven√ß√£o de problemas futuros
- Documentadas""",

    "code_generation": """Voc√™ √© um especialista em desenvolvimento de c√≥digo e programa√ß√£o.

Quando o usu√°rio pedir gera√ß√£o de c√≥digo:
1. Entenda completamente os requisitos antes de codificar
2. Escreva c√≥digo limpo, bem documentado e seguindo best practices
3. Inclua tratamento de erros e valida√ß√µes
4. Forne√ßa exemplos de uso
5. Explique a l√≥gica e decis√µes de design
6. Considere performance, seguran√ßa e escalabilidade
7. Sugira melhorias e otimiza√ß√µes
8. Forne√ßa testes quando apropriado

Foque em c√≥digo que seja:
- Funcional e testado
- Bem documentado
- Seguro e perform√°tico
- F√°cil de manter
- Seguindo padr√µes da linguagem""",

    "general": """Para conversas gerais:
1. Seja amig√°vel, profissional e extremamente prestativo
2. Mantenha o foco no objetivo do usu√°rio
3. Ofere√ßa ajuda adicional e proativa quando apropriado
4. Use linguagem clara mas t√©cnica quando necess√°rio
5. Forne√ßa contexto e explica√ß√µes quando √∫til
6. Seja conciso mas completo
7. Antecipe necessidades n√£o expressas
8. Ofere√ßa m√∫ltiplas perspectivas quando relevante""",
}

# ============================================================================
# MENSAGENS DO SISTEMA
# ============================================================================

SYSTEM_MESSAGES = {
    "welcome": "Ol√°! üëã Sou seu assistente especialista em dashboards e an√°lise de dados. Como posso ajud√°-lo hoje?",
    "thinking": "üí≠ Analisando e processando...",
    "error": "‚ùå Ocorreu um erro. Vou investigar e fornecer uma solu√ß√£o.",
    "no_response": "N√£o foi poss√≠vel gerar uma resposta. Verifique sua conex√£o com a OpenAI e tente novamente.",
    "model_not_found": "Modelo n√£o encontrado. Verifique se o modelo est√° dispon√≠vel na sua conta OpenAI.",
    "rate_limit": "Limite de requisi√ß√µes atingido. Aguarde um momento e tente novamente.",
    "insufficient_quota": "Cota insuficiente. Verifique seu plano OpenAI.",
}

# ============================================================================
# VALIDA√á√ïES E LIMITES
# ============================================================================

VALIDATION_RULES = {
    "temperature_range": (MIN_TEMPERATURE, MAX_TEMPERATURE),
    "min_message_length": 1,
    "max_message_length": 10000,
    "allowed_languages": ["pt-BR", "en-US", "es-ES"],
    "max_tokens_range": (1, 4096),  # Limite da OpenAI
    "top_p_range": (0.0, 1.0),
    "frequency_penalty_range": (-2.0, 2.0),
    "presence_penalty_range": (-2.0, 2.0),
}

# ============================================================================
# CONFIGURA√á√ïES AVAN√áADAS
# ============================================================================

ADVANCED_CONFIG = {
    # Retry em caso de falha
    "max_retries": 3,
    "retry_delay": 1.0,  # segundos
    "exponential_backoff": True,  # Backoff exponencial entre tentativas
    
    # Cache de respostas (futuro)
    "enable_cache": False,
    "cache_ttl": 3600,  # segundos
    
    # Logging
    "log_requests": True,
    "log_responses": False,  # Pode conter dados sens√≠veis
    "log_errors": True,
    
    # Performance
    "enable_streaming": True,  # OpenAI suporta streaming
    "stream_chunk_size": 50,  # Tokens por chunk no streaming
    
    # Rate limiting
    "respect_rate_limits": True,
    "requests_per_minute": 60,  # Ajustar conforme plano OpenAI
    
    # Qualidade
    "validate_responses": True,
    "check_code_syntax": True,  # Validar sintaxe de c√≥digo gerado
}

# ============================================================================
# CONFIGURA√á√ïES POR MODELO
# ============================================================================

MODEL_SPECIFIC_CONFIG = {
    "gpt-4.1": {
        "max_tokens": 4096,
        "recommended_temperature": 0.7,
        "context_length": 128000,
        "best_for": ["an√°lise avan√ßada", "racioc√≠nio complexo", "c√≥digo", "an√°lise de dados"],
    },
    "gpt-4o": {
        "max_tokens": 4096,
        "recommended_temperature": 0.7,
        "context_length": 128000,
        "best_for": ["an√°lise complexa", "c√≥digo", "racioc√≠nio"],
    },
    "gpt-4o-mini": {
        "max_tokens": 16384,
        "recommended_temperature": 0.7,
        "context_length": 128000,
        "best_for": ["an√°lise r√°pida", "respostas curtas"],
    },
    "gpt-4-turbo": {
        "max_tokens": 4096,
        "recommended_temperature": 0.7,
        "context_length": 128000,
        "best_for": ["an√°lise detalhada", "c√≥digo complexo"],
    },
    "gpt-4": {
        "max_tokens": 4096,
        "recommended_temperature": 0.7,
        "context_length": 8192,
        "best_for": ["an√°lise profunda", "racioc√≠nio complexo"],
    },
    "gpt-3.5-turbo": {
        "max_tokens": 4096,
        "recommended_temperature": 0.7,
        "context_length": 16385,
        "best_for": ["respostas r√°pidas", "tarefas gerais"],
    },
    "gpt-3.5-turbo-16k": {
        "max_tokens": 4096,
        "recommended_temperature": 0.7,
        "context_length": 16385,
        "best_for": ["contexto longo", "an√°lise de documentos"],
    },
}

# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================


def get_system_prompt(context: str = "general") -> str:
    """
    Retorna o system prompt completo com contexto espec√≠fico.

    Args:
        context: Contexto da conversa ("dashboard", "data_analysis", "error_help", "code_generation", "general")

    Returns:
        System prompt completo
    """
    base_prompt = SYSTEM_PROMPT

    if context in CONTEXT_PROMPTS:
        context_instructions = CONTEXT_PROMPTS[context]
        return f"{base_prompt}\n\nCONTEXTO ATUAL - MODO ESPECIALISTA:\n{context_instructions}"

    return base_prompt


def validate_temperature(temperature: float) -> float:
    """
    Valida e ajusta a temperatura para o range permitido.

    Args:
        temperature: Temperatura a validar

    Returns:
        Temperatura validada
    """
    min_temp, max_temp = VALIDATION_RULES["temperature_range"]
    return max(min_temp, min(max_temp, temperature))


def get_model_parameters(
    temperature: float = None,
    max_tokens: int = None,
    top_p: float = None,
    frequency_penalty: float = None,
    presence_penalty: float = None,
    model: str = None,
    **kwargs
) -> dict:
    """
    Retorna dicion√°rio com par√¢metros do modelo OpenAI.

    Par√¢metros suportados pela OpenAI:
    - temperature: Controla aleatoriedade (0.0-2.0)
    - max_tokens: M√°ximo de tokens na resposta
    - top_p: Nucleus sampling (0.0-1.0)
    - frequency_penalty: Penalidade por frequ√™ncia (-2.0 a 2.0)
    - presence_penalty: Penalidade por presen√ßa (-2.0 a 2.0)

    Args:
        temperature: Temperatura (usa padr√£o se None)
        max_tokens: M√°ximo de tokens (usa padr√£o do modelo se None)
        top_p: Top-p sampling (usa padr√£o se None)
        frequency_penalty: Penalidade por frequ√™ncia (usa padr√£o se None)
        presence_penalty: Penalidade por presen√ßa (usa padr√£o se None)
        model: Nome do modelo (para obter configura√ß√µes espec√≠ficas)
        **kwargs: Par√¢metros adicionais

    Returns:
        Dicion√°rio com par√¢metros no formato esperado pela OpenAI
    """
    params = {
        "temperature": validate_temperature(temperature or DEFAULT_TEMPERATURE),
    }

    # Obter configura√ß√µes espec√≠ficas do modelo se fornecido
    model_config = None
    if model and model in MODEL_SPECIFIC_CONFIG:
        model_config = MODEL_SPECIFIC_CONFIG[model]

    # max_tokens
    if max_tokens is not None:
        params["max_tokens"] = max_tokens
    elif model_config:
        params["max_tokens"] = model_config.get("max_tokens", DEFAULT_MAX_TOKENS)
    else:
        params["max_tokens"] = DEFAULT_MAX_TOKENS

    # top_p
    if top_p is not None:
        params["top_p"] = max(0.0, min(1.0, top_p))
    else:
        params["top_p"] = DEFAULT_TOP_P

    # frequency_penalty
    if frequency_penalty is not None:
        params["frequency_penalty"] = max(-2.0, min(2.0, frequency_penalty))
    else:
        params["frequency_penalty"] = DEFAULT_FREQUENCY_PENALTY

    # presence_penalty
    if presence_penalty is not None:
        params["presence_penalty"] = max(-2.0, min(2.0, presence_penalty))
    else:
        params["presence_penalty"] = DEFAULT_PRESENCE_PENALTY

    # Adicionar par√¢metros adicionais
    if "stop" in kwargs:
        params["stop"] = kwargs["stop"]
    if "n" in kwargs:
        params["n"] = kwargs["n"]
    if "stream" in kwargs:
        params["stream"] = kwargs["stream"]

    return params


def get_model_config(model: str) -> dict:
    """
    Retorna configura√ß√µes espec√≠ficas de um modelo.

    Args:
        model: Nome do modelo OpenAI

    Returns:
        Dicion√°rio com configura√ß√µes do modelo ou None se n√£o encontrado
    """
    return MODEL_SPECIFIC_CONFIG.get(model, {})


def get_behavior_settings() -> dict:
    """
    Retorna configura√ß√µes de comportamento do modelo.

    Returns:
        Dicion√°rio com configura√ß√µes
    """
    return BEHAVIOR_CONFIG.copy()


def get_validation_rules() -> dict:
    """
    Retorna regras de valida√ß√£o.

    Returns:
        Dicion√°rio com regras
    """
    return VALIDATION_RULES.copy()


def get_recommended_temperature(model: str, task_type: str = "general") -> float:
    """
    Retorna temperatura recomendada para um modelo e tipo de tarefa.

    Args:
        model: Nome do modelo
        task_type: Tipo de tarefa ("creative", "analytical", "code", "general")

    Returns:
        Temperatura recomendada
    """
    model_config = MODEL_SPECIFIC_CONFIG.get(model, {})
    base_temp = model_config.get("recommended_temperature", DEFAULT_TEMPERATURE)

    # Ajustar baseado no tipo de tarefa
    task_adjustments = {
        "creative": 0.9,  # Mais criativo
        "analytical": 0.3,  # Mais determin√≠stico
        "code": 0.2,  # Muito determin√≠stico para c√≥digo
        "general": 0.7,  # Balanceado
    }

    adjustment = task_adjustments.get(task_type, 0.7)
    return adjustment


def get_optimal_max_tokens(model: str, context_length: int = None) -> int:
    """
    Retorna max_tokens √≥timo baseado no modelo e contexto.

    Args:
        model: Nome do modelo
        context_length: Tamanho do contexto atual (opcional)

    Returns:
        max_tokens recomendado
    """
    model_config = MODEL_SPECIFIC_CONFIG.get(model, {})
    max_tokens = model_config.get("max_tokens", DEFAULT_MAX_TOKENS)

    # Se contexto fornecido, ajustar para deixar espa√ßo
    if context_length:
        model_max_context = model_config.get("context_length", 16385)
        # Deixar pelo menos 10% do contexto para resposta
        recommended = int(model_max_context * 0.1)
        return min(max_tokens, recommended)

    return max_tokens

