# app.py - Layout moderno com sidebar de chat
import streamlit as st
import os
import sys
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv

# Adicionar diret√≥rio raiz do projeto ao Python path
# Isso permite que os imports de src.core e src.config funcionem
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Carrega vari√°veis de ambiente do arquivo .env
load_dotenv()

# Configurar logging
try:
    from src.config.logging_config import setup_logging, get_logger

    setup_logging(level=os.getenv("LOG_LEVEL", "INFO"), log_to_console=False)
    logger = get_logger(__name__)
    logger.info("Aplica√ß√£o iniciada")
except ImportError:
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

# Imports dos m√≥dulos customizados
try:
    from src.core.llm_handler import create_llm_handler
    from src.core.openai_handler import create_openai_handler
    from src.core.audio_transcriber import transcribe_audio
    from src.config.styles import CUSTOM_CSS
    from src.config.themes import generate_theme_css
    from src.core.input_validator import validate_user_input, sanitize_input
    from src.core.history_manager import (
        save_history,
        load_history,
        list_history_sessions,
        auto_save_history,
    )
    from src.core.data_loader import load_csv_data, get_data_info, get_data_summary
    from src.core.chart_generator import (
        generate_chart_from_request,
        display_chart,
        PLOTLY_AVAILABLE,
    )

    LLM_AVAILABLE = True
    OPENAI_AVAILABLE = True
    AUDIO_AVAILABLE = True
    VALIDATION_AVAILABLE = True
    HISTORY_AVAILABLE = True
    DATA_AVAILABLE = True
    CHARTS_AVAILABLE = PLOTLY_AVAILABLE
except ImportError as e:
    LLM_AVAILABLE = False
    OPENAI_AVAILABLE = False
    AUDIO_AVAILABLE = False
    VALIDATION_AVAILABLE = False
    HISTORY_AVAILABLE = False
    DATA_AVAILABLE = False
    CHARTS_AVAILABLE = False
    CUSTOM_CSS = ""
    logger.warning(f"Alguns m√≥dulos n√£o foram encontrados: {str(e)}")
    st.warning(f"‚ö†Ô∏è Alguns m√≥dulos n√£o foram encontrados: {str(e)}")

    # Fallbacks para m√≥dulos de dados
    def load_csv_data(filepath=None):
        return None

    def get_data_summary(df):
        return "Dados n√£o dispon√≠veis."

    def generate_chart_from_request(df, chart_type, **kwargs):
        return None

    def display_chart(chart):
        st.warning("Bibliotecas de gr√°ficos n√£o dispon√≠veis")

    # Fallbacks
    def create_llm_handler(base_url=None):
        return None

    def create_openai_handler(api_key=None):
        return None

    def validate_user_input(text, **kwargs):
        return (True, None)

    def sanitize_input(text):
        return text

    def auto_save_history(messages, session_id="current"):
        pass

    def generate_theme_css(theme):
        return ""


# Configura√ß√£o da p√°gina com layout wide
st.set_page_config(
    page_title="Omnilink AI",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# CSS customizado importado de styles.py
if CUSTOM_CSS:
    st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


# Importar m√≥dulo de temas
try:
    from src.config.themes import generate_theme_css
except ImportError:
    # Fallback caso o m√≥dulo n√£o esteja dispon√≠vel
    def generate_theme_css(theme):
        return ""


# CSS din√¢mico para tema escuro/claro (mantido para compatibilidade)
def get_theme_css(theme):
    """Retorna CSS baseado no tema selecionado - DEPRECATED: use generate_theme_css do m√≥dulo themes"""
    return generate_theme_css(theme)


# Inicializar tema antes de usar
if "theme" not in st.session_state:
    st.session_state.theme = "escuro"  # "claro" ou "escuro" - padr√£o: escuro

# Aplicar CSS do tema usando o novo m√≥dulo
st.markdown(generate_theme_css(st.session_state.theme), unsafe_allow_html=True)

# CSS SIMPLIFICADO para altern√¢ncia microfone/seta
st.markdown(
    """
<style>
/* Container para posicionar elementos */
.chat-input-wrapper {
    position: relative !important;
    width: 100% !important;
}

/* Posicionar o audio input sobre o bot√£o de envio */
.chat-input-wrapper > div[data-testid="stAudioInput"] {
    position: absolute !important;
    right: 45px !important;
    top: 50% !important;
    transform: translateY(-50%) !important;
    z-index: 100 !important;
    width: auto !important;
}

/* Esconder label do audio input */
.chat-input-wrapper > div[data-testid="stAudioInput"] label {
    display: none !important;
}

/* Estilizar o bot√£o do microfone */
.chat-input-wrapper > div[data-testid="stAudioInput"] button {
    background: transparent !important;
    border: none !important;
    padding: 8px !important;
    width: 40px !important;
    height: 40px !important;
    border-radius: 50% !important;
    display: flex !important;
    align-items: center !important;
    justify-content: center !important;
    font-size: 1.3rem !important;
    color: #666 !important;
    transition: all 0.2s ease !important;
}

.chat-input-wrapper > div[data-testid="stAudioInput"] button:hover {
    background: rgba(102, 126, 234, 0.1) !important;
    color: #667eea !important;
}

/* Esconder microfone quando h√° texto */
.chat-input-wrapper.has-text > div[data-testid="stAudioInput"] {
    opacity: 0 !important;
    pointer-events: none !important;
}

/* Esconder seta de envio quando n√£o h√° texto */
.chat-input-wrapper:not(.has-text) button[data-testid="baseButton-secondary"] {
    opacity: 0 !important;
    pointer-events: none !important;
}

/* Mostrar seta quando h√° texto */
.chat-input-wrapper.has-text button[data-testid="baseButton-secondary"] {
    opacity: 1 !important;
    pointer-events: all !important;
}
</style>
""",
    unsafe_allow_html=True,
)

# JavaScript SIMPLIFICADO para detectar texto no input
st.markdown(
    """
<script>
// Fun√ß√£o para configurar a detec√ß√£o de texto
function setupTextDetection() {
    // Aguardar um pouco para garantir que os elementos estejam carregados
    setTimeout(() => {
        // Encontrar todos os inputs de chat
        const chatInputs = document.querySelectorAll('[data-testid="stChatInput"] input');
        
        chatInputs.forEach((input, index) => {
            // Encontrar o wrapper mais pr√≥ximo
            let wrapper = input.closest('.chat-input-wrapper');
            if (!wrapper) {
                // Criar wrapper se n√£o existir
                wrapper = document.createElement('div');
                wrapper.className = 'chat-input-wrapper';
                input.parentElement.parentElement.parentElement.insertBefore(wrapper, input.parentElement.parentElement);
                wrapper.appendChild(input.parentElement.parentElement);
                
                // Tentar encontrar o audio input correspondente e mover para o wrapper
                const audioInputs = document.querySelectorAll('[data-testid="stAudioInput"]');
                if (audioInputs[index]) {
                    wrapper.appendChild(audioInputs[index]);
                }
            }
            
            // Fun√ß√£o para atualizar o estado
            function updateState() {
                if (input.value.trim().length > 0) {
                    wrapper.classList.add('has-text');
                } else {
                    wrapper.classList.remove('has-text');
                }
            }
            
            // Configurar listeners
            input.addEventListener('input', updateState);
            input.addEventListener('keyup', updateState);
            input.addEventListener('change', updateState);
            
            // Estado inicial
            updateState();
        });
    }, 500);
}

// Executar quando a p√°gina carrega
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', setupTextDetection);
} else {
    setupTextDetection();
}

// Executar tamb√©m quando houver mudan√ßas (Streamlit reruns)
const observer = new MutationObserver(() => {
    setTimeout(setupTextDetection, 300);
});

observer.observe(document.body, {
    childList: true,
    subtree: true
});
</script>
""",
    unsafe_allow_html=True,
)

# Inicializa√ß√£o do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "llm_handler" not in st.session_state:
    st.session_state.llm_handler = None

# Importar configura√ß√µes do modelo
try:
    from src.config.model_config import DEFAULT_MODEL, DEFAULT_TEMPERATURE
except ImportError:
    DEFAULT_MODEL = "llama2:latest"
    DEFAULT_TEMPERATURE = 0.7

if "selected_model" not in st.session_state:
    st.session_state.selected_model = DEFAULT_MODEL

if "temperature" not in st.session_state:
    st.session_state.temperature = DEFAULT_TEMPERATURE

if "prompt_in_center" not in st.session_state:
    st.session_state.prompt_in_center = True  # Padr√£o: prompt no centro

if "audio_transcribed" not in st.session_state:
    st.session_state.audio_transcribed = None

if "is_thinking" not in st.session_state:
    st.session_state.is_thinking = False

if "ollama_url" not in st.session_state:
    st.session_state.ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

if "llm_provider" not in st.session_state:
    st.session_state.llm_provider = "ollama"  # "ollama" ou "openai"

if "transcription_method" not in st.session_state:
    st.session_state.transcription_method = os.getenv("TRANSCRIPTION_METHOD", "whisper")

# Carregar dados de ve√≠culos
if "veiculos_df" not in st.session_state:
    if DATA_AVAILABLE:
        try:
            st.session_state.veiculos_df = load_csv_data()
            if st.session_state.veiculos_df is not None:
                logger.info(f"Dados de ve√≠culos carregados: {len(st.session_state.veiculos_df)} registros")
        except Exception as e:
            logger.warning(f"Erro ao carregar dados: {e}")
            st.session_state.veiculos_df = None
    else:
        st.session_state.veiculos_df = None

# Carregar dados de ve√≠culos
if "veiculos_df" not in st.session_state:
    if DATA_AVAILABLE:
        try:
            st.session_state.veiculos_df = load_csv_data()
            if st.session_state.veiculos_df is not None:
                logger.info(f"Dados de ve√≠culos carregados: {len(st.session_state.veiculos_df)} registros")
        except Exception as e:
            logger.warning(f"Erro ao carregar dados: {e}")
            st.session_state.veiculos_df = None
    else:
        st.session_state.veiculos_df = None

# Configurar timeout (pode ser definido via vari√°vel de ambiente ou model_config)
try:
    from src.config.model_config import MODEL_RULES

    DEFAULT_TIMEOUT = MODEL_RULES.get("timeout_seconds", 120)
except ImportError:
    DEFAULT_TIMEOUT = 120

OLLAMA_TIMEOUT = int(
    os.getenv("OLLAMA_TIMEOUT", str(DEFAULT_TIMEOUT))
)  # Padr√£o vem de model_config.py ou 120 segundos

# Auto-inicializa√ß√£o do handler baseado no provedor
if st.session_state.llm_handler is None:
    try:
        if st.session_state.llm_provider == "openai":
            if OPENAI_AVAILABLE:
                st.session_state.llm_handler = create_openai_handler(timeout=OLLAMA_TIMEOUT)
        else:  # ollama
            if LLM_AVAILABLE:
                st.session_state.llm_handler = create_llm_handler(
                    st.session_state.ollama_url, timeout=OLLAMA_TIMEOUT
                )
        # Verificar conex√£o silenciosamente na inicializa√ß√£o
        if st.session_state.llm_handler:
            st.session_state.llm_handler.is_configured()
    except Exception as e:
        st.session_state.llm_handler = None
        # Log do erro (pode ser √∫til para debug)
        if "connection_debug" not in st.session_state:
            st.session_state.connection_debug = str(e)

# ========== SIDEBAR ESQUERDA - CHAT ==========
with st.sidebar:
    # Header com gradiente roxo
    st.markdown(
        """
        <div class="sidebar-header">
            <div class="sidebar-title">
                ü§ñ Omnilink AI
            </div>
            <div class="sidebar-subtitle">
                Assistente de Dashboards Inteligentes
            </div>
        </div>
        """,
        unsafe_allow_html=True,
    )

    # Mensagem de boas-vindas
    st.markdown(
        """
        <div class="welcome-message">
            <strong>Ol√°! üëã</strong><br>
            Sou seu assistente de dashboards. Pe√ßa visualiza√ß√µes de dados e eu gero para voc√™ em tempo real!
        </div>
        """,
        unsafe_allow_html=True,
    )

    # Container para hist√≥rico de mensagens (scroll√°vel)
    chat_history_container = st.container()
    with chat_history_container:
        if st.session_state.messages:
            for message in st.session_state.messages:
                if message["role"] == "user":
                    # Usar classe CSS que ser√° estilizada pelo tema
                    st.markdown(
                        f'<div class="chat-message user-message"><strong>Voc√™:</strong> {message["content"]}</div>',
                        unsafe_allow_html=True,
                    )
                else:
                    # Usar classe CSS que ser√° estilizada pelo tema
                    st.markdown(
                        f'<div class="chat-message assistant-message"><strong>Assistente:</strong> {message["content"]}</div>',
                        unsafe_allow_html=True,
                    )
        else:
            st.markdown(
                '<div class="empty-chat-message">Nenhuma mensagem ainda</div>',
                unsafe_allow_html=True,
            )

    st.markdown("<br>", unsafe_allow_html=True)

    # Mostrar indicador de pensando acima do prompt se estiver pensando
    if st.session_state.is_thinking:
        st.markdown(
            '<div class="thinking-above-prompt">üí≠ <em>Pensando...</em></div>',
            unsafe_allow_html=True,
        )

    # Container integrado para prompt e microfone - SIMPLIFICADO
    if not st.session_state.prompt_in_center:
        # Input de mensagem usando chat_input
        user_input = st.chat_input("Digite sua mensagem...", key="sidebar_chat_input")

        # Microfone posicionado via CSS
        audio_file = st.audio_input(
            "üéôÔ∏è", key="sidebar_audio", help="Clique para gravar uma mensagem de voz"
        )

        # Processar √°udio se fornecido
        if audio_file:
            st.audio(audio_file, format="audio/wav")
            with st.spinner("Transcrevendo √°udio..."):
                try:
                    if AUDIO_AVAILABLE:
                        transcribed_text = transcribe_audio(
                            audio_file, method=st.session_state.transcription_method
                        )
                        if transcribed_text:
                            st.session_state.audio_transcribed = transcribed_text
                        else:
                            st.error(
                                "‚ùå Erro ao transcrever √°udio. Verifique se o servi√ßo est√° configurado."
                            )
                    else:
                        st.warning(
                            "‚ö†Ô∏è Transcri√ß√£o de √°udio n√£o dispon√≠vel. Instale as depend√™ncias necess√°rias."
                        )
                except Exception as e:
                    st.error(f"‚ùå Erro na transcri√ß√£o: {str(e)}")

            if st.session_state.audio_transcribed:
                st.info(
                    f"üìù Transcri√ß√£o do √°udio: **{st.session_state.audio_transcribed}**"
                )
                # Se houver transcri√ß√£o, usar como input
                if not user_input:
                    user_input = st.session_state.audio_transcribed
                    st.session_state.audio_transcribed = None  # Limpar ap√≥s usar

    else:
        # Placeholder para manter estrutura quando prompt est√° no centro
        user_input = None

    st.markdown("---")

    # Configura√ß√µes (colaps√°vel)
    with st.expander("‚öôÔ∏è Configura√ß√µes"):
        # Sele√ß√£o de provedor LLM
        st.markdown("### ü§ñ Provedor de IA")
        llm_provider = st.selectbox(
            "Escolha o provedor de IA",
            ["ollama", "openai"],
            index=0 if st.session_state.llm_provider == "ollama" else 1,
            help="Ollama: modelos locais. OpenAI: modelos da OpenAI (requer API key)",
        )

        if llm_provider != st.session_state.llm_provider:
            st.session_state.llm_provider = llm_provider
            st.session_state.llm_handler = None
            st.rerun()

        # Configura√ß√£o baseada no provedor selecionado
        if st.session_state.llm_provider == "openai":
            st.markdown("### üîß Configura√ß√£o da OpenAI")
            
            # Verificar se a API key est√° configurada
            openai_key = os.getenv("OPENAI_API_KEY", "")
            if not openai_key:
                st.warning(
                    "‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada no arquivo .env. "
                    "Configure a chave no arquivo .env para usar modelos OpenAI."
                )
                st.info(
                    "üí° Para configurar: Edite o arquivo `.env` e adicione:\n"
                    "`OPENAI_API_KEY=sk-sua-chave-aqui`"
                )

            if st.button("üîÑ Conectar √† OpenAI", use_container_width=True):
                try:
                    if OPENAI_AVAILABLE:
                        # Usar API key do .env (n√£o da interface)
                        st.session_state.llm_handler = create_openai_handler(
                            api_key=None, timeout=OLLAMA_TIMEOUT
                        )
                        if (
                            st.session_state.llm_handler
                            and st.session_state.llm_handler.is_configured()
                        ):
                            st.success("‚úÖ Conectado √† OpenAI!")
                            st.rerun()
                        else:
                            st.warning("‚ö†Ô∏è OpenAI n√£o est√° dispon√≠vel. Verifique a API key.")
                    else:
                        st.error("‚ùå M√≥dulo OpenAI n√£o dispon√≠vel")
                except Exception as e:
                    st.error(f"‚ùå Erro ao conectar: {str(e)}")
                    st.session_state.llm_handler = None

        else:  # ollama
            st.markdown("### üîß Configura√ß√£o do Ollama")
            ollama_url = st.text_input(
                "URL do servidor Ollama",
                value=st.session_state.ollama_url,
                help="URL padr√£o: http://localhost:11434",
            )

            if ollama_url != st.session_state.ollama_url:
                st.session_state.ollama_url = ollama_url
                try:
                    if LLM_AVAILABLE:
                        st.session_state.llm_handler = create_llm_handler(
                            ollama_url, timeout=OLLAMA_TIMEOUT
                        )
                        if (
                            st.session_state.llm_handler
                            and st.session_state.llm_handler.is_configured()
                        ):
                            st.success("‚úÖ Conectado ao Ollama!")
                        else:
                            st.warning("‚ö†Ô∏è Ollama n√£o est√° dispon√≠vel nesta URL")
                    else:
                        st.error("‚ùå M√≥dulo Ollama n√£o dispon√≠vel")
                except Exception as e:
                    st.error(f"‚ùå Erro ao conectar: {str(e)}")
                    st.session_state.llm_handler = None

        # Status detalhado da conex√£o
        if st.session_state.llm_handler:
            status = st.session_state.llm_handler.get_connection_status()
            if status["connected"]:
                st.success(status["message"])
                if st.session_state.llm_provider == "ollama":
                    st.info(
                        f"üì° URL: {status.get('url', 'N/A')} | üì¶ Modelos: {status.get('model_count', 0)}"
                    )
                else:  # openai
                    st.info(
                        f"üì¶ Modelos dispon√≠veis: {status.get('model_count', 0)}"
                    )
                if status.get("models"):
                    st.caption(f"Modelos: {', '.join(status['models'])}")
            else:
                st.error(status["message"])
                if status.get("error"):
                    st.caption(f"Erro: {status['error']}")
                if status.get("suggestion"):
                    st.info(f"üí° {status['suggestion']}")
        else:
            st.warning("‚ö†Ô∏è Handler n√£o inicializado")

        # Bot√£o para reconectar
        provider_name = "Ollama" if st.session_state.llm_provider == "ollama" else "OpenAI"
        if st.button(f"üîÑ Reconectar ao {provider_name}", use_container_width=True):
            try:
                if st.session_state.llm_provider == "openai":
                    if OPENAI_AVAILABLE:
                        # Usar API key do .env (n√£o da interface)
                        st.session_state.llm_handler = create_openai_handler(
                            api_key=None, timeout=OLLAMA_TIMEOUT
                        )
                    else:
                        st.error("‚ùå M√≥dulo OpenAI n√£o dispon√≠vel")
                else:  # ollama
                    if LLM_AVAILABLE:
                        st.session_state.llm_handler = create_llm_handler(
                            st.session_state.ollama_url, timeout=OLLAMA_TIMEOUT
                        )
                    else:
                        st.error("‚ùå M√≥dulo Ollama n√£o dispon√≠vel")

                if st.session_state.llm_handler:
                    status = st.session_state.llm_handler.get_connection_status()
                    if status["connected"]:
                        st.success("‚úÖ Conectado com sucesso!")
                        st.rerun()
                    else:
                        st.error(f"‚ùå {status['message']}")
                        if status.get("error"):
                            st.caption(f"Detalhes: {status['error']}")
                        if status.get("suggestion"):
                            st.info(f"üí° {status['suggestion']}")
                else:
                    st.error("‚ùå N√£o foi poss√≠vel criar o handler")
            except Exception as e:
                st.error(f"‚ùå Erro: {str(e)}")
                import traceback

                with st.expander("üîç Detalhes do erro"):
                    st.code(traceback.format_exc())

        # Sele√ß√£o de modelo - buscar dinamicamente do provedor selecionado
        st.markdown("### üì¶ Sele√ß√£o de Modelo")
        try:
            if st.session_state.llm_handler:
                available_models = st.session_state.llm_handler.list_available_models()
                if available_models:
                    # Se o modelo atual n√£o est√° na lista, usar o primeiro dispon√≠vel
                    if st.session_state.selected_model not in available_models:
                        st.session_state.selected_model = available_models[0]

                    current_index = (
                        available_models.index(st.session_state.selected_model)
                        if st.session_state.selected_model in available_models
                        else 0
                    )
                    model_label = f"Modelo {provider_name}"
                    st.session_state.selected_model = st.selectbox(
                        model_label, available_models, index=current_index
                    )
                else:
                    if st.session_state.llm_provider == "ollama":
                        st.warning(
                            "‚ö†Ô∏è Nenhum modelo encontrado. Baixe modelos usando: ollama pull <nome_do_modelo>"
                        )
                    else:
                        st.warning("‚ö†Ô∏è Nenhum modelo dispon√≠vel")
                    st.session_state.selected_model = st.text_input(
                        "Digite o nome do modelo", value=st.session_state.selected_model
                    )
            else:
                st.warning(f"‚ö†Ô∏è Conecte ao {provider_name} primeiro")
                st.session_state.selected_model = st.text_input(
                    "Nome do modelo", value=st.session_state.selected_model
                )
        except Exception as e:
            st.error(f"Erro ao listar modelos: {str(e)}")
            st.session_state.selected_model = st.text_input(
                "Nome do modelo", value=st.session_state.selected_model
            )

        # Controle de temperatura
        st.session_state.temperature = st.slider(
            "Criatividade (temperature)",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1,
        )

        st.markdown("---")

        # Configura√ß√£o de transcri√ß√£o de √°udio
        st.markdown("### üéôÔ∏è Transcri√ß√£o de √Åudio")
        transcription_method = st.selectbox(
            "M√©todo de transcri√ß√£o",
            ["whisper", "openai"],
            index=0 if st.session_state.transcription_method == "whisper" else 1,
            help="Whisper: local (requer openai-whisper). OpenAI: API (requer OPENAI_API_KEY)",
        )
        st.session_state.transcription_method = transcription_method

        if transcription_method == "openai":
            openai_key = os.getenv("OPENAI_API_KEY", "")
            if not openai_key:
                st.warning("‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada no .env para transcri√ß√£o")

        st.markdown("---")

        # Informa√ß√£o sobre posi√ß√£o do prompt
        st.markdown("**üìç Posi√ß√£o do Prompt**")
        st.info(
            "üí° O prompt e o gravador de voz est√£o fixos no centro inferior da tela para melhor acessibilidade."
        )

        st.markdown("---")

        # Configura√ß√£o de tema
        st.markdown("### üé® Apar√™ncia")
        theme = st.selectbox(
            "Tema da p√°gina",
            ["escuro", "claro"],  # Escuro primeiro (padr√£o)
            index=0 if st.session_state.theme == "escuro" else 1,
            help="Escolha entre tema escuro ou claro",
        )

        if theme != st.session_state.theme:
            st.session_state.theme = theme
            st.rerun()

        # Bot√£o limpar chat
        if st.button("üóëÔ∏è Limpar Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

    st.markdown("---")

    # Status do Sistema (na sidebar)
    with st.expander("üîß Status do Sistema", expanded=False):
        # Preparar valores do status
        handler_available = "‚úÖ Sim" if LLM_AVAILABLE else "‚ùå N√£o"
        handler_available_color = "#28a745" if LLM_AVAILABLE else "#dc3545"

        handler_initialized = (
            "‚úÖ Sim" if st.session_state.llm_handler is not None else "‚ùå N√£o"
        )
        handler_initialized_color = (
            "#28a745" if st.session_state.llm_handler is not None else "#dc3545"
        )

        ollama_configured = (
            "‚úÖ Sim"
            if (
                st.session_state.llm_handler
                and st.session_state.llm_handler.is_configured()
            )
            else "‚ùå N√£o"
        )
        ollama_configured_color = (
            "#28a745"
            if (
                st.session_state.llm_handler
                and st.session_state.llm_handler.is_configured()
            )
            else "#dc3545"
        )

        audio_available = "‚úÖ Sim" if AUDIO_AVAILABLE else "‚ùå N√£o"
        audio_available_color = "#28a745" if AUDIO_AVAILABLE else "#dc3545"

        num_messages = len(st.session_state.messages)
        current_model = st.session_state.selected_model

        st.markdown(
            f"""
            <div class="status-container">
                <div class="status-item">
                    <span class="status-label">Handler Dispon√≠vel</span>
                    <span class="status-value" style="color: {handler_available_color};">
                        {handler_available}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Handler Inicializado</span>
                    <span class="status-value" style="color: {handler_initialized_color};">
                        {handler_initialized}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Ollama Conectado</span>
                    <span class="status-value" style="color: {ollama_configured_color};">
                        {ollama_configured}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Transcri√ß√£o de √Åudio</span>
                    <span class="status-value" style="color: {audio_available_color};">
                        {audio_available}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Mensagens no Hist√≥rico</span>
                    <span class="status-value" style="color: #667eea;">
                        {num_messages}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Modelo Atual</span>
                    <span class="status-value" style="color: #667eea;">
                        {current_model}
                    </span>
                </div>
            </div>
            """,
            unsafe_allow_html=True,
        )

# Processar mensagem quando enviada (texto ou √°udio transcrito)
if "user_input" in locals() and user_input:
    if (
        st.session_state.llm_handler is None
        or not st.session_state.llm_handler.is_configured()
    ):
        st.error("‚ö†Ô∏è Ollama n√£o est√° dispon√≠vel. Verifique se o servidor est√° rodando.")
        logger.warning("Tentativa de enviar mensagem sem Ollama configurado")
    else:
        # Validar input do usu√°rio
        if VALIDATION_AVAILABLE:
            user_input = sanitize_input(user_input)
            is_valid, error = validate_user_input(user_input)
            if not is_valid:
                st.error(f"‚ùå {error}")
                logger.warning(f"Input do usu√°rio rejeitado: {error}")
                st.stop()

        logger.info(f"Mensagem do usu√°rio recebida: {len(user_input)} caracteres")

        # Adicionar mensagem do usu√°rio no hist√≥rico
        st.session_state.messages.append({"role": "user", "content": user_input})

        # Exibir imediatamente a mensagem do usu√°rio
        with st.chat_message("user"):
            st.markdown(user_input)

        # Gerar resposta
        with st.chat_message("assistant"):
            placeholder = st.empty()
            full_response = ""

            try:
                # Gerar resposta (streaming ser√° implementado futuramente)
                response = st.session_state.llm_handler.generate_response(
                    messages=st.session_state.messages,
                    model=st.session_state.selected_model,
                    temperature=st.session_state.temperature,
                    stream=False,  # Streaming pode ser ativado aqui
                )

                placeholder.markdown(response)
                full_response = response
                logger.info(f"Resposta gerada: {len(full_response)} caracteres")
            except Exception as e:
                error_msg = f"Erro ao gerar resposta: {str(e)}"
                placeholder.error(error_msg)
                logger.error(error_msg, exc_info=True)
                full_response = error_msg

        # Salvar no hist√≥rico
        st.session_state.messages.append(
            {"role": "assistant", "content": full_response}
        )

        # Salvar hist√≥rico automaticamente
        if HISTORY_AVAILABLE:
            try:
                auto_save_history(st.session_state.messages, "current")
            except Exception as e:
                logger.warning(f"Erro ao salvar hist√≥rico: {e}")

        # Recarregar para atualizar a interface
        st.rerun()

# ========== √ÅREA PRINCIPAL - DASHBOARD ==========
# √Årea principal para exibir conte√∫do/dashboards
main_area = st.container()

with main_area:
    if not st.session_state.messages:
        # Estado vazio - mostrar √≠cone e mensagem
        st.markdown(
            """
            <div class="empty-dashboard">
                <div class="dashboard-icon">
                    <svg viewBox="0 0 200 150" xmlns="http://www.w3.org/2000/svg">
                        <!-- Ret√¢ngulo principal com bordas arredondadas -->
                        <rect x="10" y="10" width="180" height="130" rx="8" ry="8" 
                              fill="none" stroke="#999" stroke-width="2"/>
                        <!-- Painel superior horizontal -->
                        <rect x="20" y="20" width="160" height="30" rx="4" ry="4" 
                              fill="#e0e0e0" stroke="#ccc" stroke-width="1"/>
                        <!-- Painel esquerdo vertical -->
                        <rect x="20" y="60" width="70" height="70" rx="4" ry="4" 
                              fill="#e0e0e0" stroke="#ccc" stroke-width="1"/>
                        <!-- Painel inferior direito (quadrado) -->
                        <rect x="100" y="100" width="80" height="30" rx="4" ry="4" 
                              fill="#e0e0e0" stroke="#ccc" stroke-width="1"/>
                        <!-- Painel superior direito -->
                        <rect x="100" y="60" width="80" height="30" rx="4" ry="4" 
                              fill="#e0e0e0" stroke="#ccc" stroke-width="1"/>
                    </svg>
                </div>
                <div class="empty-text">Nenhum dashboard gerado</div>
                <div class="empty-text-secondary">Envie uma mensagem no chat para come√ßar</div>
            </div>
            """,
            unsafe_allow_html=True,
        )
    else:
        # Exibir conte√∫do das respostas
        # Se a √∫ltima mensagem for do assistente, mostrar em destaque
        if (
            st.session_state.messages
            and st.session_state.messages[-1]["role"] == "assistant"
        ):
            last_response = st.session_state.messages[-1]["content"]

            # Container para resposta
            st.markdown("### üìä Resposta do Assistente")
            st.markdown("---")

            # Exibir resposta formatada
            response_container = st.container()
            with response_container:
                st.markdown(last_response)

                # Tentar gerar gr√°fico se o usu√°rio solicitou
                if DATA_AVAILABLE and CHARTS_AVAILABLE and st.session_state.veiculos_df is not None:
                    try:
                        from src.core.chart_analyzer import create_smart_chart, detect_chart_request
                        
                        # Verificar √∫ltima mensagem do usu√°rio
                        if len(st.session_state.messages) >= 2:
                            last_user_message = st.session_state.messages[-2].get("content", "")
                            
                            # Detectar se √© solicita√ß√£o de gr√°fico
                            chart_request = detect_chart_request(last_user_message)
                            if chart_request:
                                st.markdown("---")
                                st.markdown("### üìà Visualiza√ß√£o Gerada")
                                
                                # Criar gr√°fico inteligente
                                chart = create_smart_chart(st.session_state.veiculos_df, last_user_message)
                                if chart:
                                    display_chart(chart)
                                else:
                                    # Tentar gr√°fico padr√£o
                                    from src.core.chart_generator import create_bar_chart
                                    df = st.session_state.veiculos_df
                                    if "cidade" in df.columns and "km_mes" in df.columns:
                                        # Agrupar por cidade
                                        df_grouped = df.groupby("cidade")["km_mes"].sum().reset_index()
                                        chart = create_bar_chart(df_grouped, x="cidade", y="km_mes", 
                                                                title="Quilometragem Total por Cidade")
                                        if chart:
                                            display_chart(chart)
                    except Exception as e:
                        logger.warning(f"Erro ao gerar gr√°fico: {e}")

                # Bot√µes de a√ß√£o - apenas √≠cones
                col1, col2, col3 = st.columns(3)

                with col1:
                    if st.button("üîÑ", use_container_width=True, key="btn_regenerar"):
                        # Remove √∫ltima resposta e regenera
                        if len(st.session_state.messages) >= 2:
                            st.session_state.messages.pop()
                            st.session_state.messages.pop()
                            st.rerun()

                with col2:
                    if st.button("üìã", use_container_width=True, key="btn_copiar"):
                        st.success("Resposta copiada!")

                with col3:
                    if st.button("üóëÔ∏è", use_container_width=True, key="btn_limpar"):
                        st.session_state.messages = []
                        st.rerun()

        # Hist√≥rico completo (colaps√°vel)
        if len(st.session_state.messages) > 2:
            with st.expander("üìú Hist√≥rico Completo da Conversa"):
                for i, message in enumerate(st.session_state.messages):
                    role_icon = "üë§" if message["role"] == "user" else "ü§ñ"
                    st.markdown(f"**{role_icon} {message['role'].title()}:**")
                    st.markdown(message["content"])
                    if i < len(st.session_state.messages) - 1:
                        st.markdown("---")

    # Prompt e gravador sempre no centro, abaixo da tela
    st.markdown("<br><br>", unsafe_allow_html=True)  # Espa√ßo antes do prompt

    # Mostrar indicador de pensando acima do prompt se estiver pensando
    if st.session_state.is_thinking:
        st.markdown(
            '<div class="thinking-above-prompt">üí≠ <em>Pensando...</em></div>',
            unsafe_allow_html=True,
        )

    # Input de mensagem usando chat_input no centro
    center_user_input = st.chat_input(
        "Digite sua mensagem...", key="center_chat_input"
    )

    # Microfone no centro
    center_audio_file = st.audio_input(
        "üéôÔ∏è", key="center_audio", help="Clique para gravar uma mensagem de voz"
    )

    # Processar √°udio se fornecido
    if center_audio_file:
        st.audio(center_audio_file, format="audio/wav")
        with st.spinner("Transcrevendo √°udio..."):
            try:
                if AUDIO_AVAILABLE:
                    transcribed_text = transcribe_audio(
                        center_audio_file,
                        method=st.session_state.transcription_method,
                    )
                    if transcribed_text:
                        st.session_state.audio_transcribed = transcribed_text
                    else:
                        st.error(
                            "‚ùå Erro ao transcrever √°udio. Verifique se o servi√ßo est√° configurado."
                        )
                else:
                    st.warning(
                        "‚ö†Ô∏è Transcri√ß√£o de √°udio n√£o dispon√≠vel. Instale as depend√™ncias necess√°rias."
                    )
            except Exception as e:
                st.error(f"‚ùå Erro na transcri√ß√£o: {str(e)}")

        if st.session_state.audio_transcribed:
            st.info(
                f"üìù Transcri√ß√£o do √°udio: **{st.session_state.audio_transcribed}**"
            )
            # Se houver transcri√ß√£o, usar como input
            if not center_user_input:
                center_user_input = st.session_state.audio_transcribed
                st.session_state.audio_transcribed = None  # Limpar ap√≥s usar

    # Processar mensagem quando enviada do centro
    if center_user_input:
        if (
            st.session_state.llm_handler is None
            or not st.session_state.llm_handler.is_configured()
        ):
            st.error(
                "‚ö†Ô∏è Provedor de IA n√£o est√° dispon√≠vel. Verifique a conex√£o nas configura√ß√µes."
            )
            logger.warning("Tentativa de enviar mensagem sem handler configurado")
        else:
            # Validar input do usu√°rio
            if VALIDATION_AVAILABLE:
                center_user_input = sanitize_input(center_user_input)
                is_valid, error = validate_user_input(center_user_input)
                if not is_valid:
                    st.error(f"‚ùå {error}")
                    logger.warning(f"Input do usu√°rio rejeitado: {error}")
                    st.stop()

            logger.info(
                f"Mensagem do usu√°rio recebida (centro): {len(center_user_input)} caracteres"
            )

            # Adicionar mensagem do usu√°rio no hist√≥rico
            st.session_state.messages.append(
                {"role": "user", "content": center_user_input}
            )

            # Exibir imediatamente a mensagem do usu√°rio
            with st.chat_message("user"):
                st.markdown(center_user_input)

            # Gerar resposta
            with st.chat_message("assistant"):
                placeholder = st.empty()
                full_response = ""

                try:
                    # Gerar resposta
                    response = st.session_state.llm_handler.generate_response(
                        messages=st.session_state.messages,
                        model=st.session_state.selected_model,
                        temperature=st.session_state.temperature,
                        stream=False,
                    )

                    placeholder.markdown(response)
                    full_response = response
                    logger.info(f"Resposta gerada: {len(full_response)} caracteres")
                except Exception as e:
                    error_msg = f"Erro ao gerar resposta: {str(e)}"
                    placeholder.error(error_msg)
                    logger.error(error_msg, exc_info=True)
                    full_response = error_msg

            # Salvar no hist√≥rico
            st.session_state.messages.append(
                {"role": "assistant", "content": full_response}
            )

            # Salvar hist√≥rico automaticamente
            if HISTORY_AVAILABLE:
                try:
                    auto_save_history(st.session_state.messages, "current")
                except Exception as e:
                    logger.warning(f"Erro ao salvar hist√≥rico: {e}")

            # Recarregar para atualizar a interface
            st.rerun()
