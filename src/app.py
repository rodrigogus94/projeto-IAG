"""
app.py - Aplica√ß√£o principal do Omnilink AI

Interface web moderna constru√≠da com Streamlit para chat com IA,
gera√ß√£o de gr√°ficos e an√°lise de dados.

Funcionalidades:
- Chat conversacional com m√∫ltiplos provedores LLM (Ollama/OpenAI)
- Gera√ß√£o autom√°tica de gr√°ficos e visualiza√ß√µes
- Transcri√ß√£o de √°udio (Whisper/OpenAI)
- Gerenciamento de hist√≥rico de conversas
- Interface responsiva com temas escuro/claro
"""

import streamlit as st
import os
import sys
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv

# Adicionar diret√≥rio raiz do projeto ao Python path
# Isso permite que os imports de src.core e src.config funcionem
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Carrega vari√°veis de ambiente do arquivo .env
# Trata erros de codifica√ß√£o (arquivo pode estar em UTF-16)
try:
    load_dotenv()
except UnicodeDecodeError:
    # Se houver erro de codifica√ß√£o, tenta recarregar especificando encoding
    import io
    env_path = project_root / ".env"
    if env_path.exists():
        try:
            # Tenta ler com diferentes encodings
            for encoding in ['utf-8', 'utf-16', 'latin-1']:
                try:
                    with open(env_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    # Reescreve o arquivo em UTF-8
                    with open(env_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    # Recarrega
                    load_dotenv()
                    break
                except (UnicodeDecodeError, UnicodeError):
                    continue
        except Exception:
            # Se n√£o conseguir corrigir, apenas ignora o arquivo .env
            pass
    # Se n√£o conseguir carregar, continua sem o arquivo .env
    pass
except Exception:
    # Outros erros s√£o ignorados silenciosamente
    pass

# Configurar logging
try:
    from src.config.logging_config import setup_logging, get_logger

    setup_logging(level=os.getenv("LOG_LEVEL", "INFO"), log_to_console=False)
    logger = get_logger(__name__)
    logger.info("Aplica√ß√£o iniciada")
except ImportError:
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

# Imports dos m√≥dulos customizados
try:
    from src.core.llm_handler import create_llm_handler
    from src.core.openai_handler import create_openai_handler
    from src.core.audio_transcriber import transcribe_audio
    from src.config.styles import CUSTOM_CSS
    from src.config.themes import generate_theme_css
    from src.core.input_validator import validate_user_input, sanitize_input
    from src.core.history_manager import (
        save_history,
        load_history,
        list_history_sessions,
        auto_save_history,
    )
    from src.core.data_loader import load_csv_data, get_data_info, get_data_summary
    from src.core.chart_generator import (
        generate_chart_from_request,
        display_chart,
        PLOTLY_AVAILABLE,
    )

    LLM_AVAILABLE = True
    OPENAI_AVAILABLE = True
    AUDIO_AVAILABLE = True
    VALIDATION_AVAILABLE = True
    HISTORY_AVAILABLE = True
    DATA_AVAILABLE = True
    CHARTS_AVAILABLE = PLOTLY_AVAILABLE
except ImportError as e:
    LLM_AVAILABLE = False
    OPENAI_AVAILABLE = False
    AUDIO_AVAILABLE = False
    VALIDATION_AVAILABLE = False
    HISTORY_AVAILABLE = False
    DATA_AVAILABLE = False
    CHARTS_AVAILABLE = False
    CUSTOM_CSS = ""
    logger.warning(f"Alguns m√≥dulos n√£o foram encontrados: {str(e)}")
    st.warning(f"‚ö†Ô∏è Alguns m√≥dulos n√£o foram encontrados: {str(e)}")

    # Fallbacks para m√≥dulos de dados
    def load_csv_data(filepath=None):
        return None

    def get_data_summary(df):
        return "Dados n√£o dispon√≠veis."

    def generate_chart_from_request(df, chart_type, **kwargs):
        return None

    def display_chart(chart):
        st.warning("Bibliotecas de gr√°ficos n√£o dispon√≠veis")

    # Fallbacks
    def create_llm_handler(base_url=None):
        return None

    def create_openai_handler(api_key=None):
        return None

    def validate_user_input(text, **kwargs):
        return (True, None)

    def sanitize_input(text):
        return text

    def auto_save_history(messages, session_id="current"):
        pass

    def generate_theme_css(theme):
        return ""


# ============================================================================
# CONSTANTES E CONFIGURA√á√ïES
# ============================================================================

# Configura√ß√£o da p√°gina (deve estar antes de st.set_page_config)
PAGE_CONFIG = {
    "page_title": "Omnilink AI",
    "page_icon": "ü§ñ",
    "layout": "wide",
    "initial_sidebar_state": "expanded",
}

# Configura√ß√£o da p√°gina
st.set_page_config(**PAGE_CONFIG)

# Meta tag viewport para responsividade em mobile
st.markdown(
    """
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    """,
    unsafe_allow_html=True,
)

# CSS customizado importado de styles.py
if CUSTOM_CSS:
    st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


# Importar m√≥dulo de temas
try:
    from src.config.themes import generate_theme_css
except ImportError:
    # Fallback caso o m√≥dulo n√£o esteja dispon√≠vel
    def generate_theme_css(theme):
        return ""


# ============================================================================
# FUN√á√ïES DE COMPATIBILIDADE
# ============================================================================

def get_theme_css(theme):
    """
    Retorna CSS baseado no tema selecionado.
    
    DEPRECATED: Use generate_theme_css do m√≥dulo themes.
    Mantido para compatibilidade com c√≥digo existente.
    """
    return generate_theme_css(theme)


# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================

def process_audio_file(audio_file, transcription_method):
    """
    Processa um arquivo de √°udio e retorna o texto transcrito.
    
    Args:
        audio_file: Arquivo de √°udio do Streamlit
        transcription_method: M√©todo de transcri√ß√£o ("whisper" ou "openai")
    
    Returns:
        Texto transcrito ou None em caso de erro
    """
    if not audio_file:
        return None
    
    try:
        if AUDIO_AVAILABLE:
            transcribed_text = transcribe_audio(audio_file, method=transcription_method)
            if transcribed_text:
                return transcribed_text
            else:
                st.error("‚ùå Erro ao transcrever √°udio. Verifique se o servi√ßo est√° configurado.")
                return None
        else:
            st.warning("‚ö†Ô∏è Transcri√ß√£o de √°udio n√£o dispon√≠vel. Instale as depend√™ncias necess√°rias.")
            return None
    except Exception as e:
        st.error(f"‚ùå Erro na transcri√ß√£o: {str(e)}")
        logger.error(f"Erro ao transcrever √°udio: {str(e)}", exc_info=True)
        return None


def process_user_message(user_input):
    """
    Processa uma mensagem do usu√°rio: valida, adiciona ao hist√≥rico, gera resposta e salva.
    
    Args:
        user_input: Texto da mensagem do usu√°rio
    """
    if not user_input or not user_input.strip():
        return
    
    # Verificar se o handler est√° configurado
    if st.session_state.llm_handler is None or not st.session_state.llm_handler.is_configured():
        provider_name = "Ollama" if st.session_state.llm_provider == "ollama" else "OpenAI"
        st.error(f"‚ö†Ô∏è {provider_name} n√£o est√° dispon√≠vel. Verifique a conex√£o nas configura√ß√µes.")
        logger.warning(f"Tentativa de enviar mensagem sem {provider_name} configurado")
        return
    
    # Validar input do usu√°rio
    if VALIDATION_AVAILABLE:
        user_input = sanitize_input(user_input)
        is_valid, error = validate_user_input(user_input)
        if not is_valid:
            st.error(f"‚ùå {error}")
            logger.warning(f"Input do usu√°rio rejeitado: {error}")
            return
    
    logger.info(f"Mensagem do usu√°rio recebida: {len(user_input)} caracteres")
    
    # Adicionar mensagem do usu√°rio no hist√≥rico
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # Exibir imediatamente a mensagem do usu√°rio
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # Gerar resposta
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        
        try:
            # Preparar contexto dos dados se dispon√≠vel
            messages_to_send = st.session_state.messages.copy()
            
            # Adicionar contexto dos dados na primeira mensagem
            if len(messages_to_send) == 1 and st.session_state.veiculos_df is not None:
                df = st.session_state.veiculos_df
                data_context = f"""[DADOS DISPON√çVEIS - Arquivo: dados_veiculos_300.csv]
    Total: {len(df)} ve√≠culos | Colunas: {', '.join(df.columns.tolist())}
    Status: Ativos={len(df[df['status'] == 'ativo'])}, Manuten√ß√£o={len(df[df['status'] == 'manutencao'])}, Inativos={len(df[df['status'] == 'inativo'])}

    [INSTRU√á√ÉO CR√çTICA] Este √© um sistema web que gera gr√°ficos AUTOMATICAMENTE. NUNCA forne√ßa c√≥digo Python. Apenas analise os dados e apresente insights. O gr√°fico j√° aparece sozinho na tela.

    Pergunta: {user_input}"""
                
                messages_to_send[0] = {"role": "user", "content": data_context}
            
            # SEMPRE verificar se √© pedido de gr√°fico e refor√ßar a instru√ß√£o
            user_input_lower = user_input.lower()
            if any(palavra in user_input_lower for palavra in ['gr√°fico', 'grafico', 'chart', 'visualiza√ß√£o', 'visualizacao', 'plot']):
                # Modificar a √∫ltima mensagem do usu√°rio para incluir a instru√ß√£o
                ultima_msg = messages_to_send[-1]["content"]
                messages_to_send[-1]["content"] = f"""üö® IMPORTANTE: O sistema J√Å gera o gr√°fico automaticamente. N√ÉO forne√ßa c√≥digo. Apenas analise os dados.

    {ultima_msg}

    Responda APENAS com an√°lise dos dados (n√∫meros, percentuais, insights). O gr√°fico aparece sozinho."""
            
            # Gerar resposta
            response = st.session_state.llm_handler.generate_response(
                messages=messages_to_send,
                model=st.session_state.selected_model,
                temperature=st.session_state.temperature,
                stream=False,
            )
            
            placeholder.markdown(response)
            full_response = response
            logger.info(f"Resposta gerada: {len(full_response)} caracteres")
        except Exception as e:
            error_msg = f"Erro ao gerar resposta: {str(e)}"
            placeholder.error(error_msg)
            logger.error(error_msg, exc_info=True)
            full_response = error_msg
    
    # Salvar no hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    
    # Salvar hist√≥rico automaticamente
    if HISTORY_AVAILABLE:
        try:
            auto_save_history(st.session_state.messages, "current")
        except Exception as e:
            logger.warning(f"Erro ao salvar hist√≥rico: {e}")
    
    # Recarregar para atualizar a interface
    st.rerun()


def initialize_session_state():
    """Inicializa todas as vari√°veis do session_state com valores padr√£o."""
    defaults = {
        "messages": [],
        "llm_handler": None,
        "selected_model": DEFAULT_MODEL,
        "temperature": DEFAULT_TEMPERATURE,
        "prompt_in_center": True,
        "audio_transcribed": None,
        "is_thinking": False,
        "ollama_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
        "llm_provider": "ollama",
        "transcription_method": os.getenv("TRANSCRIPTION_METHOD", "whisper"),
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


def initialize_llm_handler():
    """Inicializa o handler LLM baseado no provedor selecionado."""
    if st.session_state.llm_handler is not None:
        return
    
    try:
        if st.session_state.llm_provider == "openai":
            if OPENAI_AVAILABLE:
                st.session_state.llm_handler = create_openai_handler(timeout=OLLAMA_TIMEOUT)
        else:  # ollama
            if LLM_AVAILABLE:
                st.session_state.llm_handler = create_llm_handler(
                    st.session_state.ollama_url, timeout=OLLAMA_TIMEOUT
                )
        
        # Verificar conex√£o silenciosamente na inicializa√ß√£o
        if st.session_state.llm_handler:
            st.session_state.llm_handler.is_configured()
    except Exception as e:
        st.session_state.llm_handler = None
        # Log do erro (pode ser √∫til para debug)
        if "connection_debug" not in st.session_state:
            st.session_state.connection_debug = str(e)
        logger.warning(f"Erro ao inicializar handler LLM: {str(e)}")


def render_chart_if_requested():
    """
<<<<<<< HEAD
    Detecta se o usu√°rio solicitou um gr√°fico e renderiza se apropriado.
    """
    if not (DATA_AVAILABLE and CHARTS_AVAILABLE and st.session_state.veiculos_df is not None):
        return
    
    try:
        from src.core.chart_analyzer import create_smart_chart, detect_chart_request
        from src.core.chart_generator import create_bar_chart
        
        # Verificar √∫ltima mensagem do usu√°rio
        if len(st.session_state.messages) < 2:
            return
        
        last_user_message = st.session_state.messages[-2].get("content", "")
        
        # Detectar se √© solicita√ß√£o de gr√°fico
        chart_request = detect_chart_request(last_user_message)
        if not chart_request:
            return
        
        st.markdown("---")
        st.markdown("### üìà Visualiza√ß√£o Gerada")
        
        # Criar gr√°fico inteligente
        chart = create_smart_chart(st.session_state.veiculos_df, last_user_message)
        if chart:
            display_chart(chart)
        else:
            # Tentar gr√°fico padr√£o
            df = st.session_state.veiculos_df
            if "cidade" in df.columns and "km_mes" in df.columns:
                # Agrupar por cidade
                df_grouped = df.groupby("cidade")["km_mes"].sum().reset_index()
                chart = create_bar_chart(
                    df_grouped, 
                    x="cidade", 
                    y="km_mes", 
                    title="Quilometragem Total por Cidade"
                )
                if chart:
                    display_chart(chart)
    except Exception as e:
        logger.warning(f"Erro ao gerar gr√°fico: {e}")

# Inicializar tema antes de usar
if "theme" not in st.session_state:
    st.session_state.theme = "escuro"  # "claro" ou "escuro" - padr√£o: escuro

# Aplicar CSS do tema usando o novo m√≥dulo
st.markdown(generate_theme_css(st.session_state.theme), unsafe_allow_html=True)

# CSS SIMPLIFICADO para altern√¢ncia microfone/seta
st.markdown(
    """
<style>
/* Container para posicionar elementos */
.chat-input-wrapper {
    position: relative !important;
    width: 100% !important;
}

/* Posicionar o audio input sobre o bot√£o de envio */
.chat-input-wrapper > div[data-testid="stAudioInput"] {
    position: absolute !important;
    right: 45px !important;
    top: 50% !important;
    transform: translateY(-50%) !important;
    z-index: 100 !important;
    width: auto !important;
}

/* Esconder label do audio input */
.chat-input-wrapper > div[data-testid="stAudioInput"] label {
    display: none !important;
}

/* Estilizar o bot√£o do microfone */
.chat-input-wrapper > div[data-testid="stAudioInput"] button {
    background: transparent !important;
    border: none !important;
    padding: 8px !important;
    width: 40px !important;
    height: 40px !important;
    border-radius: 50% !important;
    display: flex !important;
    align-items: center !important;
    justify-content: center !important;
    font-size: 1.3rem !important;
    color: #666 !important;
    transition: all 0.2s ease !important;
}

.chat-input-wrapper > div[data-testid="stAudioInput"] button:hover {
    background: rgba(102, 126, 234, 0.1) !important;
    color: #667eea !important;
}

/* Esconder microfone quando h√° texto */
.chat-input-wrapper.has-text > div[data-testid="stAudioInput"] {
    opacity: 0 !important;
    pointer-events: none !important;
}

/* Esconder seta de envio quando n√£o h√° texto */
.chat-input-wrapper:not(.has-text) button[data-testid="baseButton-secondary"] {
    opacity: 0 !important;
    pointer-events: none !important;
}

/* Mostrar seta quando h√° texto */
.chat-input-wrapper.has-text button[data-testid="baseButton-secondary"] {
    opacity: 1 !important;
    pointer-events: all !important;
}
</style>
""",
    unsafe_allow_html=True,
)

# JavaScript SIMPLIFICADO para detectar texto no input
st.markdown(
    """
<script>
// Fun√ß√£o para configurar a detec√ß√£o de texto
function setupTextDetection() {
    // Aguardar um pouco para garantir que os elementos estejam carregados
    setTimeout(() => {
        // Encontrar todos os inputs de chat
        const chatInputs = document.querySelectorAll('[data-testid="stChatInput"] input');
        
        chatInputs.forEach((input, index) => {
            // Encontrar o wrapper mais pr√≥ximo
            let wrapper = input.closest('.chat-input-wrapper');
            if (!wrapper) {
                // Criar wrapper se n√£o existir
                wrapper = document.createElement('div');
                wrapper.className = 'chat-input-wrapper';
                input.parentElement.parentElement.parentElement.insertBefore(wrapper, input.parentElement.parentElement);
                wrapper.appendChild(input.parentElement.parentElement);
                
                // Tentar encontrar o audio input correspondente e mover para o wrapper
                const audioInputs = document.querySelectorAll('[data-testid="stAudioInput"]');
                if (audioInputs[index]) {
                    wrapper.appendChild(audioInputs[index]);
                }
            }
            
            // Fun√ß√£o para atualizar o estado
            function updateState() {
                if (input.value.trim().length > 0) {
                    wrapper.classList.add('has-text');
                } else {
                    wrapper.classList.remove('has-text');
                }
            }
            
            // Configurar listeners
            input.addEventListener('input', updateState);
            input.addEventListener('keyup', updateState);
            input.addEventListener('change', updateState);
            
            // Estado inicial
            updateState();
        });
    }, 500);
}

// Executar quando a p√°gina carrega
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', setupTextDetection);
} else {
    setupTextDetection();
}

// Executar tamb√©m quando houver mudan√ßas (Streamlit reruns)
const observer = new MutationObserver(() => {
    setTimeout(setupTextDetection, 300);
});

observer.observe(document.body, {
    childList: true,
    subtree: true
});

// ============================================================================
// Fun√ß√£o para detectar se √© mobile
// ============================================================================
function isMobile() {
    return window.innerWidth <= 600;
}

function isTablet() {
    return window.innerWidth > 600 && window.innerWidth <= 1024;
}

// ============================================================================
// Fun√ß√£o para ajustar layout quando sidebar √© escondida/exibida
// ============================================================================
function adjustLayoutForSidebar() {
    const sidebar = document.querySelector('[data-testid="stSidebar"]');
    const main = document.querySelector('.main');
    const blockContainer = document.querySelector('.main .block-container');
    
    if (!sidebar || !main) return;
    
    const isExpanded = sidebar.getAttribute('aria-expanded') === 'true';
    const mobile = isMobile();
    const tablet = isTablet();
    
    if (mobile) {
        // Em mobile, sidebar sempre overlay
        if (blockContainer) {
            blockContainer.style.maxWidth = '100%';
            blockContainer.style.width = '100%';
            blockContainer.style.paddingLeft = '0.8rem';
            blockContainer.style.paddingRight = '0.8rem';
            blockContainer.style.boxSizing = 'border-box';
        }
        
        main.style.marginLeft = '0';
        main.style.marginRight = '0';
        main.style.width = '100%';
        main.style.maxWidth = '100%';
        
        // Se sidebar aberta, escurecer main
        if (isExpanded) {
            main.style.opacity = '0.3';
            main.style.pointerEvents = 'none';
        } else {
            main.style.opacity = '1';
            main.style.pointerEvents = 'auto';
        }
    } else if (tablet) {
        // Em tablet
        if (isExpanded) {
            if (blockContainer) {
                blockContainer.style.maxWidth = 'calc(100% - 350px)';
                blockContainer.style.width = 'calc(100% - 350px)';
            }
            main.style.marginLeft = '';
            main.style.width = '';
            main.style.maxWidth = '';
        } else {
            if (blockContainer) {
                blockContainer.style.maxWidth = '100%';
                blockContainer.style.width = '100%';
                blockContainer.style.paddingLeft = '1.5rem';
                blockContainer.style.paddingRight = '1.5rem';
            }
            main.style.marginLeft = '0';
            main.style.width = '100%';
            main.style.maxWidth = '100%';
        }
        main.style.opacity = '1';
        main.style.pointerEvents = 'auto';
    } else {
        // Desktop
        if (isExpanded) {
            if (blockContainer) {
                blockContainer.style.maxWidth = 'calc(1400px - 420px)';
                blockContainer.style.width = 'calc(100% - 420px)';
            }
            main.style.marginLeft = '';
            main.style.width = '';
            main.style.maxWidth = '';
            main.style.left = '';
            main.style.right = '';
        } else {
            if (blockContainer) {
                blockContainer.style.maxWidth = '100%';
                blockContainer.style.width = '100%';
                blockContainer.style.paddingLeft = '2rem';
                blockContainer.style.paddingRight = '2rem';
                blockContainer.style.boxSizing = 'border-box';
            }
            
            main.style.marginLeft = '0';
            main.style.marginRight = '0';
            main.style.width = '100vw';
            main.style.maxWidth = '100vw';
            main.style.left = '0';
            main.style.right = '0';
        }
        main.style.opacity = '1';
        main.style.pointerEvents = 'auto';
    }
    
    // Prevenir overflow horizontal
    document.body.style.overflowX = 'hidden';
    document.documentElement.style.overflowX = 'hidden';
}

// Observar mudan√ßas no estado da sidebar
function setupSidebarObserver() {
    const sidebar = document.querySelector('[data-testid="stSidebar"]');
    if (!sidebar) {
        setTimeout(setupSidebarObserver, 300);
        return;
    }
    
    // Ajustar layout inicialmente
    adjustLayoutForSidebar();
    
    // Observar mudan√ßas no atributo aria-expanded
    const sidebarObserver = new MutationObserver((mutations) => {
        mutations.forEach((mutation) => {
            if (mutation.type === 'attributes' && mutation.attributeName === 'aria-expanded') {
                setTimeout(adjustLayoutForSidebar, 50);
            }
        });
    });
    
    sidebarObserver.observe(sidebar, {
        attributes: true,
        attributeFilter: ['aria-expanded', 'class']
    });
    
    // Observar mudan√ßas no DOM para detectar quando sidebar √© escondida/exibida
    const mainObserver = new MutationObserver(() => {
        setTimeout(adjustLayoutForSidebar, 50);
    });
    
    mainObserver.observe(document.body, {
        childList: true,
        subtree: true,
        attributes: true,
        attributeFilter: ['aria-expanded', 'class', 'style']
    });
    
    // Observar mudan√ßas de tamanho da janela (debounce)
    let resizeTimeout;
    window.addEventListener('resize', () => {
        clearTimeout(resizeTimeout);
        resizeTimeout = setTimeout(() => {
            adjustLayoutForSidebar();
        }, 150);
    });
    
    // Ajustar ao mudar orienta√ß√£o (mobile)
    window.addEventListener('orientationchange', () => {
        setTimeout(adjustLayoutForSidebar, 300);
    });
    
    // Observar cliques no bot√£o da sidebar
    const sidebarButton = document.querySelector('[data-testid="stSidebar"] button, [data-testid="baseButton-header"]');
    if (sidebarButton) {
        sidebarButton.addEventListener('click', () => {
            setTimeout(adjustLayoutForSidebar, 200);
        });
    }
}

// Executar quando a p√°gina carrega
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', () => {
        setupSidebarObserver();
        setTimeout(adjustLayoutForSidebar, 500);
    });
} else {
    setupSidebarObserver();
    setTimeout(adjustLayoutForSidebar, 500);
}

// Executar tamb√©m ap√≥s delays para garantir que elementos estejam carregados
setTimeout(() => {
    setupSidebarObserver();
    adjustLayoutForSidebar();
}, 1000);

setTimeout(() => {
    adjustLayoutForSidebar();
}, 2000);
</script>
""",
    unsafe_allow_html=True,
)

# Importar configura√ß√µes do modelo
try:
    from src.config.model_config import DEFAULT_MODEL, DEFAULT_TEMPERATURE
except ImportError:
    DEFAULT_MODEL = "llama2:latest"
    DEFAULT_TEMPERATURE = 0.7

# Inicializa√ß√£o do session_state
initialize_session_state()

# Carregar dados de ve√≠culos
if "veiculos_df" not in st.session_state:
    if DATA_AVAILABLE:
        try:
            st.session_state.veiculos_df = load_csv_data()
            if st.session_state.veiculos_df is not None:
                logger.info(f"Dados de ve√≠culos carregados: {len(st.session_state.veiculos_df)} registros")
        except Exception as e:
            logger.warning(f"Erro ao carregar dados: {e}")
            st.session_state.veiculos_df = None
    else:
        st.session_state.veiculos_df = None

# Configurar timeout (pode ser definido via vari√°vel de ambiente ou model_config)
try:
    from src.config.model_config import MODEL_RULES

    DEFAULT_TIMEOUT = MODEL_RULES.get("timeout_seconds", 120)
except ImportError:
    DEFAULT_TIMEOUT = 120

OLLAMA_TIMEOUT = int(
    os.getenv("OLLAMA_TIMEOUT", str(DEFAULT_TIMEOUT))
)  # Padr√£o vem de model_config.py ou 120 segundos

# Auto-inicializa√ß√£o do handler baseado no provedor
initialize_llm_handler()

# ========== SIDEBAR ESQUERDA - CHAT ==========
with st.sidebar:
    # Header com gradiente roxo
    st.markdown(
        """
        <div class="sidebar-header">
            <div class="sidebar-title">
                ü§ñ Omnilink AI
            </div>
            <div class="sidebar-subtitle">
                Assistente de Dashboards Inteligentes
            </div>
        </div>
        """,
        unsafe_allow_html=True,
    )

    # Mensagem de boas-vindas
    st.markdown(
        """
        <div class="welcome-message">
            <strong>Ol√°! üëã</strong><br>
            Sou seu assistente de dashboards. Pe√ßa visualiza√ß√µes de dados e eu gero para voc√™ em tempo real!
        </div>
        """,
        unsafe_allow_html=True,
    )

    # Container para hist√≥rico de mensagens (scroll√°vel)
    chat_history_container = st.container()
    with chat_history_container:
        if st.session_state.messages:
            for message in st.session_state.messages:
                if message["role"] == "user":
                    # Usar classe CSS que ser√° estilizada pelo tema
                    st.markdown(
                        f'<div class="chat-message user-message"><strong>Voc√™:</strong> {message["content"]}</div>',
                        unsafe_allow_html=True,
                    )
                else:
                    # Usar classe CSS que ser√° estilizada pelo tema
                    st.markdown(
                        f'<div class="chat-message assistant-message"><strong>Assistente:</strong> {message["content"]}</div>',
                        unsafe_allow_html=True,
                    )
        else:
            st.markdown(
                '<div class="empty-chat-message">Nenhuma mensagem ainda</div>',
                unsafe_allow_html=True,
            )

    st.markdown("<br>", unsafe_allow_html=True)

    # Mostrar indicador de pensando acima do prompt se estiver pensando
    if st.session_state.is_thinking:
        st.markdown(
            '<div class="thinking-above-prompt">üí≠ <em>Pensando...</em></div>',
            unsafe_allow_html=True,
        )

    # Container integrado para prompt e microfone - SIMPLIFICADO
    if not st.session_state.prompt_in_center:
        # Input de mensagem usando chat_input
        user_input = st.chat_input("Digite sua mensagem...", key="sidebar_chat_input")

        # Microfone posicionado via CSS
        audio_file = st.audio_input(
            "üéôÔ∏è", key="sidebar_audio", help="Clique para gravar uma mensagem de voz"
        )

        # Processar √°udio se fornecido
        if audio_file:
            st.audio(audio_file, format="audio/wav")
            with st.spinner("Transcrevendo √°udio..."):
                transcribed_text = process_audio_file(audio_file, st.session_state.transcription_method)
                if transcribed_text:
                    st.session_state.audio_transcribed = transcribed_text

            if st.session_state.audio_transcribed:
                st.info(
                    f"üìù Transcri√ß√£o do √°udio: **{st.session_state.audio_transcribed}**"
                )
                # Se houver transcri√ß√£o, usar como input
                if not user_input:
                    user_input = st.session_state.audio_transcribed
                    st.session_state.audio_transcribed = None  # Limpar ap√≥s usar

    else:
        # Placeholder para manter estrutura quando prompt est√° no centro
        user_input = None

    st.markdown("---")

    # Configura√ß√µes (colaps√°vel)
    with st.expander("‚öôÔ∏è Configura√ß√µes"):
        # Sele√ß√£o de provedor LLM
        st.markdown("### ü§ñ Provedor de IA")
        llm_provider = st.selectbox(
            "Escolha o provedor de IA",
            ["ollama", "openai"],
            index=0 if st.session_state.llm_provider == "ollama" else 1,
            help="Ollama: modelos locais. OpenAI: modelos da OpenAI (requer API key)",
        )

        if llm_provider != st.session_state.llm_provider:
            st.session_state.llm_provider = llm_provider
            st.session_state.llm_handler = None
            st.rerun()

        # Configura√ß√£o baseada no provedor selecionado
        if st.session_state.llm_provider == "openai":
            st.markdown("### üîß Configura√ß√£o da OpenAI")
            
            # Verificar se a API key est√° configurada
            openai_key = os.getenv("OPENAI_API_KEY", "")
            if not openai_key:
                st.warning(
                    "‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada no arquivo .env. "
                    "Configure a chave no arquivo .env para usar modelos OpenAI."
                )
                st.info(
                    "üí° Para configurar: Edite o arquivo `.env` e adicione:\n"
                    "`OPENAI_API_KEY=sk-sua-chave-aqui`"
                )

            if st.button("üîÑ Conectar √† OpenAI", use_container_width=True):
                try:
                    if OPENAI_AVAILABLE:
                        # Usar API key do .env (n√£o da interface)
                        st.session_state.llm_handler = create_openai_handler(
                            api_key=None, timeout=OLLAMA_TIMEOUT
                        )
                        if (
                            st.session_state.llm_handler
                            and st.session_state.llm_handler.is_configured()
                        ):
                            st.success("‚úÖ Conectado √† OpenAI!")
                            st.rerun()
                        else:
                            st.warning("‚ö†Ô∏è OpenAI n√£o est√° dispon√≠vel. Verifique a API key.")
                    else:
                        st.error("‚ùå M√≥dulo OpenAI n√£o dispon√≠vel")
                except Exception as e:
                    st.error(f"‚ùå Erro ao conectar: {str(e)}")
                    st.session_state.llm_handler = None

        else:  # ollama
            st.markdown("### üîß Configura√ß√£o do Ollama")
            ollama_url = st.text_input(
                "URL do servidor Ollama",
                value=st.session_state.ollama_url,
                help="URL padr√£o: http://localhost:11434",
            )

            if ollama_url != st.session_state.ollama_url:
                st.session_state.ollama_url = ollama_url
                try:
                    if LLM_AVAILABLE:
                        st.session_state.llm_handler = create_llm_handler(
                            ollama_url, timeout=OLLAMA_TIMEOUT
                        )
                        if (
                            st.session_state.llm_handler
                            and st.session_state.llm_handler.is_configured()
                        ):
                            st.success("‚úÖ Conectado ao Ollama!")
                        else:
                            st.warning("‚ö†Ô∏è Ollama n√£o est√° dispon√≠vel nesta URL")
                    else:
                        st.error("‚ùå M√≥dulo Ollama n√£o dispon√≠vel")
                except Exception as e:
                    st.error(f"‚ùå Erro ao conectar: {str(e)}")
                    st.session_state.llm_handler = None

        # Status detalhado da conex√£o
        if st.session_state.llm_handler:
            status = st.session_state.llm_handler.get_connection_status()
            if status["connected"]:
                st.success(status["message"])
                if st.session_state.llm_provider == "ollama":
                    st.info(
                        f"üì° URL: {status.get('url', 'N/A')} | üì¶ Modelos: {status.get('model_count', 0)}"
                    )
                else:  # openai
                    st.info(
                        f"üì¶ Modelos dispon√≠veis: {status.get('model_count', 0)}"
                    )
                if status.get("models"):
                    st.caption(f"Modelos: {', '.join(status['models'])}")
            else:
                st.error(status["message"])
                if status.get("error"):
                    st.caption(f"Erro: {status['error']}")
                if status.get("suggestion"):
                    st.info(f"üí° {status['suggestion']}")
        else:
            st.warning("‚ö†Ô∏è Handler n√£o inicializado")

        # Bot√£o para reconectar
        provider_name = "Ollama" if st.session_state.llm_provider == "ollama" else "OpenAI"
        if st.button(f"üîÑ Reconectar ao {provider_name}", use_container_width=True):
            try:
                if st.session_state.llm_provider == "openai":
                    if OPENAI_AVAILABLE:
                        # Usar API key do .env (n√£o da interface)
                        st.session_state.llm_handler = create_openai_handler(
                            api_key=None, timeout=OLLAMA_TIMEOUT
                        )
                    else:
                        st.error("‚ùå M√≥dulo OpenAI n√£o dispon√≠vel")
                else:  # ollama
                    if LLM_AVAILABLE:
                        st.session_state.llm_handler = create_llm_handler(
                            st.session_state.ollama_url, timeout=OLLAMA_TIMEOUT
                        )
                    else:
                        st.error("‚ùå M√≥dulo Ollama n√£o dispon√≠vel")

                if st.session_state.llm_handler:
                    status = st.session_state.llm_handler.get_connection_status()
                    if status["connected"]:
                        st.success("‚úÖ Conectado com sucesso!")
                        st.rerun()
                    else:
                        st.error(f"‚ùå {status['message']}")
                        if status.get("error"):
                            st.caption(f"Detalhes: {status['error']}")
                        if status.get("suggestion"):
                            st.info(f"üí° {status['suggestion']}")
                else:
                    st.error("‚ùå N√£o foi poss√≠vel criar o handler")
            except Exception as e:
                st.error(f"‚ùå Erro: {str(e)}")
                import traceback

                with st.expander("üîç Detalhes do erro"):
                    st.code(traceback.format_exc())

        # Sele√ß√£o de modelo - buscar dinamicamente do provedor selecionado
        st.markdown("### üì¶ Sele√ß√£o de Modelo")
        try:
            if st.session_state.llm_handler:
                available_models = st.session_state.llm_handler.list_available_models()
                if available_models:
                    # Se o modelo atual n√£o est√° na lista, usar o primeiro dispon√≠vel
                    if st.session_state.selected_model not in available_models:
                        st.session_state.selected_model = available_models[0]

                    current_index = (
                        available_models.index(st.session_state.selected_model)
                        if st.session_state.selected_model in available_models
                        else 0
                    )
                    model_label = f"Modelo {provider_name}"
                    st.session_state.selected_model = st.selectbox(
                        model_label, available_models, index=current_index
                    )
                else:
                    if st.session_state.llm_provider == "ollama":
                        st.warning(
                            "‚ö†Ô∏è Nenhum modelo encontrado. Baixe modelos usando: ollama pull <nome_do_modelo>"
                        )
                    else:
                        st.warning("‚ö†Ô∏è Nenhum modelo dispon√≠vel")
                    st.session_state.selected_model = st.text_input(
                        "Digite o nome do modelo", value=st.session_state.selected_model
                    )
            else:
                st.warning(f"‚ö†Ô∏è Conecte ao {provider_name} primeiro")
                st.session_state.selected_model = st.text_input(
                    "Nome do modelo", value=st.session_state.selected_model
                )
        except Exception as e:
            st.error(f"Erro ao listar modelos: {str(e)}")
            st.session_state.selected_model = st.text_input(
                "Nome do modelo", value=st.session_state.selected_model
            )

        # Controle de temperatura
        st.session_state.temperature = st.slider(
            "Criatividade (temperature)",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1,
        )

        st.markdown("---")

        # Configura√ß√£o de transcri√ß√£o de √°udio
        st.markdown("### üéôÔ∏è Transcri√ß√£o de √Åudio")
        transcription_method = st.selectbox(
            "M√©todo de transcri√ß√£o",
            ["whisper", "openai"],
            index=0 if st.session_state.transcription_method == "whisper" else 1,
            help="Whisper: local (requer openai-whisper). OpenAI: API (requer OPENAI_API_KEY)",
        )
        st.session_state.transcription_method = transcription_method

        if transcription_method == "openai":
            openai_key = os.getenv("OPENAI_API_KEY", "")
            if not openai_key:
                st.warning("‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada no .env para transcri√ß√£o")

        st.markdown("---")

        # Informa√ß√£o sobre posi√ß√£o do prompt
        st.markdown("**üìç Posi√ß√£o do Prompt**")
        st.info(
            "üí° O prompt e o gravador de voz est√£o fixos no centro inferior da tela para melhor acessibilidade."
        )

        st.markdown("---")

        # Configura√ß√£o de tema
        st.markdown("### üé® Apar√™ncia")
        theme = st.selectbox(
            "Tema da p√°gina",
            ["escuro", "claro"],  # Escuro primeiro (padr√£o)
            index=0 if st.session_state.theme == "escuro" else 1,
            help="Escolha entre tema escuro ou claro",
        )

        if theme != st.session_state.theme:
            st.session_state.theme = theme
            st.rerun()

        # Bot√£o limpar chat
        if st.button("üóëÔ∏è Limpar Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

    st.markdown("---")

    # Status do Sistema (na sidebar)
    with st.expander("üîß Status do Sistema", expanded=False):
        # Preparar valores do status
        handler_available = "‚úÖ Sim" if LLM_AVAILABLE else "‚ùå N√£o"
        handler_available_color = "#28a745" if LLM_AVAILABLE else "#dc3545"

        handler_initialized = (
            "‚úÖ Sim" if st.session_state.llm_handler is not None else "‚ùå N√£o"
        )
        handler_initialized_color = (
            "#28a745" if st.session_state.llm_handler is not None else "#dc3545"
        )

        ollama_configured = (
            "‚úÖ Sim"
            if (
                st.session_state.llm_handler
                and st.session_state.llm_handler.is_configured()
            )
            else "‚ùå N√£o"
        )
        ollama_configured_color = (
            "#28a745"
            if (
                st.session_state.llm_handler
                and st.session_state.llm_handler.is_configured()
            )
            else "#dc3545"
        )

        audio_available = "‚úÖ Sim" if AUDIO_AVAILABLE else "‚ùå N√£o"
        audio_available_color = "#28a745" if AUDIO_AVAILABLE else "#dc3545"

        num_messages = len(st.session_state.messages)
        current_model = st.session_state.selected_model

        st.markdown(
            f"""
            <div class="status-container">
                <div class="status-item">
                    <span class="status-label">Handler Dispon√≠vel</span>
                    <span class="status-value" style="color: {handler_available_color};">
                        {handler_available}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Handler Inicializado</span>
                    <span class="status-value" style="color: {handler_initialized_color};">
                        {handler_initialized}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Ollama Conectado</span>
                    <span class="status-value" style="color: {ollama_configured_color};">
                        {ollama_configured}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Transcri√ß√£o de √Åudio</span>
                    <span class="status-value" style="color: {audio_available_color};">
                        {audio_available}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Mensagens no Hist√≥rico</span>
                    <span class="status-value" style="color: #667eea;">
                        {num_messages}
                    </span>
                </div>
                <div class="status-item">
                    <span class="status-label">Modelo Atual</span>
                    <span class="status-value" style="color: #667eea;">
                        {current_model}
                    </span>
                </div>
            </div>
            """,
            unsafe_allow_html=True,
        )

# Processar mensagem quando enviada da sidebar (texto ou √°udio transcrito)
if "user_input" in locals() and user_input:
    process_user_message(user_input)

# ========== √ÅREA PRINCIPAL - DASHBOARD ==========
# √Årea principal para exibir conte√∫do/dashboards
main_area = st.container()

with main_area:
    if not st.session_state.messages:
        # Estado vazio - mostrar √≠cone e mensagem
        st.markdown(
            """
            <div class="empty-dashboard">
                <div class="dashboard-icon">
                    <svg viewBox="0 0 200 150" xmlns="http://www.w3.org/2000/svg">
                        <!-- Ret√¢ngulo principal com bordas arredondadas -->
                        <rect x="10" y="10" width="180" height="130" rx="8" ry="8" 
                              fill="none" stroke="#999" stroke-width="2"/>
                        <!-- Painel superior horizontal -->
                        <rect x="20" y="20" width="160" height="30" rx="4" ry="4" 
                              fill="#e0e0e0" stroke="#ccc" stroke-width="1"/>
                        <!-- Painel esquerdo vertical -->
                        <rect x="20" y="60" width="70" height="70" rx="4" ry="4" 
                              fill="#e0e0e0" stroke="#ccc" stroke-width="1"/>
                        <!-- Painel inferior direito (quadrado) -->
                        <rect x="100" y="100" width="80" height="30" rx="4" ry="4" 
                              fill="#e0e0e0" stroke="#ccc" stroke-width="1"/>
                        <!-- Painel superior direito -->
                        <rect x="100" y="60" width="80" height="30" rx="4" ry="4" 
                              fill="#e0e0e0" stroke="#ccc" stroke-width="1"/>
                    </svg>
                </div>
                <div class="empty-text">Nenhum dashboard gerado</div>
                <div class="empty-text-secondary">Envie uma mensagem no chat para come√ßar</div>
            </div>
            """,
            unsafe_allow_html=True,
        )
    else:
        # Exibir conte√∫do das respostas
        # Se a √∫ltima mensagem for do assistente, mostrar em destaque
        if (
            st.session_state.messages
            and st.session_state.messages[-1]["role"] == "assistant"
        ):
            last_response = st.session_state.messages[-1]["content"]

            # Container para resposta
            st.markdown("### üìä Resposta do Assistente")
            st.markdown("---")

            # Exibir resposta formatada
            response_container = st.container()
            with response_container:
                st.markdown(last_response)

<<<<<<< HEAD
                # Tentar gerar gr√°fico se o usu√°rio solicitou
                render_chart_if_requested()

                # Bot√µes de a√ß√£o - apenas √≠cones
                col1, col2, col3 = st.columns(3)

                with col1:
                    if st.button("üîÑ", use_container_width=True, key="btn_regenerar"):
                        # Remove √∫ltima resposta e regenera
                        if len(st.session_state.messages) >= 2:
                            st.session_state.messages.pop()
                            st.session_state.messages.pop()
                            st.rerun()

                with col2:
                    if st.button("üìã", use_container_width=True, key="btn_copiar"):
                        st.success("Resposta copiada!")

                with col3:
                    if st.button("üóëÔ∏è", use_container_width=True, key="btn_limpar"):
                        st.session_state.messages = []
                        st.rerun()

=======
            # GERAR GR√ÅFICO AUTOMATICAMENTE (FORA do response_container)
            render_chart_if_requested()

            # Bot√µes de a√ß√£o - apenas √≠cones
            col1, col2, col3 = st.columns(3)

            with col1:
                if st.button("üîÑ", use_container_width=True, key="btn_regenerar"):
                    # Remove √∫ltima resposta e regenera
                    if len(st.session_state.messages) >= 2:
                        st.session_state.messages.pop()
                        st.session_state.messages.pop()
                        st.rerun()

            with col2:
                if st.button("üìã", use_container_width=True, key="btn_copiar"):
                    st.success("Resposta copiada!")

            with col3:
                if st.button("üóëÔ∏è", use_container_width=True, key="btn_limpar"):
                    st.session_state.messages = []
                    st.rerun()

>>>>>>> 9ff461a1e44d6fbdeb3e94597c4e3346c0321e91
        # Hist√≥rico completo (colaps√°vel)
        if len(st.session_state.messages) > 2:
            with st.expander("üìú Hist√≥rico Completo da Conversa"):
                for i, message in enumerate(st.session_state.messages):
                    role_icon = "üë§" if message["role"] == "user" else "ü§ñ"
                    st.markdown(f"**{role_icon} {message['role'].title()}:**")
                    st.markdown(message["content"])
                    if i < len(st.session_state.messages) - 1:
                        st.markdown("---")

    # Prompt e gravador sempre no centro, abaixo da tela
    st.markdown("<br><br>", unsafe_allow_html=True)  # Espa√ßo antes do prompt

    # Mostrar indicador de pensando acima do prompt se estiver pensando
    if st.session_state.is_thinking:
        st.markdown(
            '<div class="thinking-above-prompt">üí≠ <em>Pensando...</em></div>',
            unsafe_allow_html=True,
        )

    # Input de mensagem usando chat_input no centro
    center_user_input = st.chat_input(
        "Digite sua mensagem...", key="center_chat_input"
    )

    # Microfone no centro
    center_audio_file = st.audio_input(
        "üéôÔ∏è", key="center_audio", help="Clique para gravar uma mensagem de voz"
    )

    # Processar √°udio se fornecido
    if center_audio_file:
        st.audio(center_audio_file, format="audio/wav")
        with st.spinner("Transcrevendo √°udio..."):
            transcribed_text = process_audio_file(center_audio_file, st.session_state.transcription_method)
            if transcribed_text:
                st.session_state.audio_transcribed = transcribed_text

        if st.session_state.audio_transcribed:
            st.info(
                f"üìù Transcri√ß√£o do √°udio: **{st.session_state.audio_transcribed}**"
            )
            # Se houver transcri√ß√£o, usar como input
            if not center_user_input:
                center_user_input = st.session_state.audio_transcribed
                st.session_state.audio_transcribed = None  # Limpar ap√≥s usar

    # Processar mensagem quando enviada do centro
    if center_user_input:
        process_user_message(center_user_input)
